<html>
<head>
  <title>Evernote Export</title>
  <basefont face="Tahoma" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/276742; Windows/6.1.7601 Service Pack 1 (Win64);"/>
  <style>
    body, td {
      font-family: Tahoma;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="301"/>

<div>
<span><div>结合以上两步: 模拟x和a的joint probability distribution: p(x, a)<br clear="none"/><br clear="none"/>Generative learning: 模拟输入数据的概率分部<br clear="none"/>discriminative learning: 把输入映射到输出, 区分几类点 <br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image.png" type="image/png" alt="Alt text"/><br clear="none"/><br clear="none"/> <a href="https://www.quora.com/What-is-a-good-laymans-explanation-for-the-Kullback-Leibler-Divergence" shape="rect" target="_blank">Kullback Leibler Divergence</a><br clear="none"/><br clear="none"/>p: 原始数据分布<br clear="none"/>q: 重新建设的概率分部<br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [1].png" type="image/png" alt="Alt text"/><br clear="none"/><br clear="none"/><br clear="none"/>概率分部:<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><img alt="Alt text" src="https://upload.wikimedia.org/wikipedia/commons/1/12/Dice_Distribution_%28bar%29.svg"></img><br clear="none"/><br clear="none"/>对于图像一样, 像素点也有一定的概率分布:<br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [2].png" type="image/png" alt="Alt text"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image.jpg" type="image/jpeg" alt="Alt text"/><br clear="none"/><br clear="none"/><br clear="none"/>正向更新: 给定这些像素, 权重应该送出一个更强的信号给大象还是狗?<br clear="none"/><br clear="none"/>反向更新: 给定大象和狗, 我应该期待什么样的像素分布?<br clear="none"/><br clear="none"/>多层<br clear="none"/><br clear="none"/><br clear="none"/>例子:<br clear="none"/><br clear="none"/>&quot;&quot;&quot;<br clear="none"/>==============================================================<br clear="none"/>Restricted Boltzmann Machine features for digit classification<br clear="none"/>==============================================================<br clear="none"/><br clear="none"/>For greyscale image data where pixel values can be interpreted as degrees of<br clear="none"/>blackness on a white background, like handwritten digit recognition, the<br clear="none"/>Bernoulli Restricted Boltzmann machine model (:class:`BernoulliRBM<br clear="none"/>&lt;sklearn.neural_network.BernoulliRBM&gt;`) can perform effective non-linear<br clear="none"/>feature extraction.<br clear="none"/><br clear="none"/>In order to learn good latent representations from a small dataset, we<br clear="none"/>artificially generate more labeled data by perturbing the training data with<br clear="none"/>linear shifts of 1 pixel in each direction.<br clear="none"/><br clear="none"/>This example shows how to build a classification pipeline with a BernoulliRBM<br clear="none"/>feature extractor and a :class:`LogisticRegression<br clear="none"/>&lt;sklearn.linear_model.LogisticRegression&gt;` classifier. The hyperparameters<br clear="none"/>of the entire model (learning rate, hidden layer size, regularization)<br clear="none"/>were optimized by grid search, but the search is not reproduced here because<br clear="none"/>of runtime constraints.<br clear="none"/><br clear="none"/>Logistic regression on raw pixel values is presented for comparison. The<br clear="none"/>example shows that the features extracted by the BernoulliRBM help improve the<br clear="none"/>classification accuracy.<br clear="none"/>&quot;&quot;&quot;<br clear="none"/><br clear="none"/>from __future__ import print_function<br clear="none"/><br clear="none"/>print(__doc__)<br clear="none"/><br clear="none"/># Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve<br clear="none"/># License: BSD<br clear="none"/><br clear="none"/>import numpy as np<br clear="none"/>import matplotlib.pyplot as plt<br clear="none"/><br clear="none"/>from scipy.ndimage import convolve<br clear="none"/>from sklearn import linear_model, datasets, metrics<br clear="none"/>from sklearn.cross_validation import train_test_split<br clear="none"/>from sklearn.neural_network import BernoulliRBM<br clear="none"/>from sklearn.pipeline import Pipeline<br clear="none"/><br clear="none"/><br clear="none"/>###############################################################################<br clear="none"/># Setting up<br clear="none"/><br clear="none"/>def nudge_dataset(X, Y):<br clear="none"/> &quot;&quot;&quot;<br clear="none"/> This produces a dataset 5 times bigger than the original one,<br clear="none"/> by moving the 8x8 images in X around by 1px to left, right, down, up<br clear="none"/> &quot;&quot;&quot;<br clear="none"/> direction_vectors = [<br clear="none"/> [[0, 1, 0],<br clear="none"/> [0, 0, 0],<br clear="none"/> [0, 0, 0]],<br clear="none"/><br clear="none"/> [[0, 0, 0],<br clear="none"/> [1, 0, 0],<br clear="none"/> [0, 0, 0]],<br clear="none"/><br clear="none"/> [[0, 0, 0],<br clear="none"/> [0, 0, 1],<br clear="none"/> [0, 0, 0]],<br clear="none"/><br clear="none"/> [[0, 0, 0],<br clear="none"/> [0, 0, 0],<br clear="none"/> [0, 1, 0]]]<br clear="none"/><br clear="none"/> shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',<br clear="none"/> weights=w).ravel()<br clear="none"/> X = np.concatenate([X] +<br clear="none"/> [np.apply_along_axis(shift, 1, X, vector)<br clear="none"/> for vector in direction_vectors])<br clear="none"/> Y = np.concatenate([Y for _ in range(5)], axis=0)<br clear="none"/> return X, Y<br clear="none"/><br clear="none"/># Load Data<br clear="none"/>digits = datasets.load_digits()<br clear="none"/>X = np.asarray(digits.data, 'float32')<br clear="none"/>X, Y = nudge_dataset(X, digits.target)<br clear="none"/>X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001) # 0-1 scaling<br clear="none"/><br clear="none"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y,<br clear="none"/> test_size=0.2,<br clear="none"/> random_state=0)<br clear="none"/><br clear="none"/># Models we will use<br clear="none"/>logistic = linear_model.LogisticRegression()<br clear="none"/>rbm = BernoulliRBM(random_state=0, verbose=True)<br clear="none"/><br clear="none"/>classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])<br clear="none"/><br clear="none"/>###############################################################################<br clear="none"/># Training<br clear="none"/><br clear="none"/># Hyper-parameters. These were set by cross-validation,<br clear="none"/># using a GridSearchCV. Here we are not performing cross-validation to<br clear="none"/># save time.<br clear="none"/>rbm.learning_rate = 0.06<br clear="none"/>rbm.n_iter = 20<br clear="none"/># More components tend to give better prediction performance, but larger<br clear="none"/># fitting time<br clear="none"/>rbm.n_components = 100<br clear="none"/>logistic.C = 6000.0<br clear="none"/><br clear="none"/># Training RBM-Logistic Pipeline<br clear="none"/>classifier.fit(X_train, Y_train)<br clear="none"/><br clear="none"/># Training Logistic regression<br clear="none"/>logistic_classifier = linear_model.LogisticRegression(C=100.0)<br clear="none"/>logistic_classifier.fit(X_train, Y_train)<br clear="none"/><br clear="none"/>###############################################################################<br clear="none"/># Evaluation<br clear="none"/><br clear="none"/>print()<br clear="none"/>print(&quot;Logistic regression using RBM features:\n%s\n&quot; % (<br clear="none"/> metrics.classification_report(<br clear="none"/> Y_test,<br clear="none"/> classifier.predict(X_test))))<br clear="none"/><br clear="none"/>print(&quot;Logistic regression using raw pixel features:\n%s\n&quot; % (<br clear="none"/> metrics.classification_report(<br clear="none"/> Y_test,<br clear="none"/> logistic_classifier.predict(X_test))))<br clear="none"/><br clear="none"/>###############################################################################<br clear="none"/># Plotting<br clear="none"/><br clear="none"/>plt.figure(figsize=(4.2, 4))<br clear="none"/>for i, comp in enumerate(rbm.components_):<br clear="none"/> plt.subplot(10, 10, i + 1)<br clear="none"/> plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,<br clear="none"/> interpolation='nearest')<br clear="none"/> plt.xticks(())<br clear="none"/> plt.yticks(())<br clear="none"/>plt.suptitle('100 components extracted by RBM', fontsize=16)<br clear="none"/>plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)<br clear="none"/><br clear="none"/>plt.show()<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/></div></span>
</div>
<hr>
<a name="306"/>

<div>
<span><div>回顾上节课讲的CNN的实现: network3.py</div><div><br clear="none"/></div><div><div>Trying to run under a GPU.  If this is not desired, then modify network3.py</div><div>to set the GPU flag to False.</div><div>Training mini-batch number 0</div><div>Training mini-batch number 1000</div><div>.</div><div>.</div><div>Training mini-batch number 24000<br clear="none"/></div><div>Epoch 0: validation accuracy 98.82%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 98.96%</div><div>Training mini-batch number 25000</div><div>.</div><div>.</div><div>Training mini-batch number 49000</div><div>Epoch 1: validation accuracy 98.95%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 99.21%</div><div>Training mini-batch number 50000</div><div><br clear="none"/></div><div>Training mini-batch number 74000</div><div>Epoch 2: validation accuracy 99.21%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 99.25%</div><div>Training mini-batch number 75000</div><div>.</div><div>.</div><div>Training mini-batch number 99000</div><div>Epoch 3: validation accuracy 99.13%</div><div>.</div><div>.</div><div>Training mini-batch number 124000</div><div>Epoch 4: validation accuracy 99.33%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 99.51%</div><div>.</div><div>.</div><div>Training mini-batch number 174000</div><div>Epoch 6: validation accuracy 99.33%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 99.53%</div><div>Training mini-batch number 175000</div><div>.</div><div>.</div><div>Training mini-batch number 199000</div><div>Epoch 7: validation accuracy 99.32%</div><div>Training mini-batch number 200000</div><div>.</div><div>.</div><div>Training mini-batch number 224000</div><div>Epoch 8: validation accuracy 99.35%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 99.49%</div><div>Training mini-batch number 225000</div><div>.</div><div>.</div><div>Training mini-batch number 249000</div><div>Epoch 9: validation accuracy 99.35%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 99.50%</div><div>.</div><div>.</div><div>Training mini-batch number 274000</div><div>Epoch 10: validation accuracy 99.35%</div><div>This is the best validation accuracy to date.</div><div>The corresponding test accuracy is 99.53%</div><div><br clear="none"/></div></div><div><br clear="none"/></div><div><strong>Restricted Boltzmann Machine:</strong></div><div><br clear="none"/></div><div>Geoff Hinton发明</div><div><br clear="none"/></div><div>降低维度, 分类, 回归, 特征学习</div><div><br clear="none"/></div><div>非监督学习(unsupervised learning)</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-deeplearning4j.org 2015-09-27 20-17-48.png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><pre xml:space="preserve"><code>activation f((weight w * input x) + bias b ) = output a<br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [3].png" type="image/png" alt="Alt text"/><br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [4].png" type="image/png" alt="Alt text"/><br clear="none"/><br clear="none"/>多个输入:<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [5].png" type="image/png" alt="Alt text"/><br clear="none"/><br clear="none"/>多个隐藏层:<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [6].png" type="image/png" alt="Alt text"/><br clear="none"/>Reconstructions:<br clear="none"/><br clear="none"/>隐藏层变成输入层, 反向更新, 用老的权重和新的bias:<br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [7].png" type="image/png" alt="Alt text"/><br clear="none"/>回到原始输入层:<br clear="none"/>算出的值跟原始输入层的值比较, 最小化error, 接着迭代更新:<br clear="none"/><br clear="none"/><br clear="none"/>正向更新: 用输入预测神经元的activation, 也就是输出的概率, 在给定的权重下: <span>p(a|x; w)</span><br clear="none"/><br clear="none"/>反向更新的时候:<br clear="none"/><br clear="none"/>activation被输入到网络里面,来预测原始的数据X, RBM尝试估计X的概率, 对于给定的activation a: p(x|a; w)<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/></code><br clear="none"/></pre></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="313"/>

<div>
<span><div><strong>Deep Brief Network</strong>: 多个Restricted Boltzmann Machines</div><div><br clear="none"/></div><div>每层的神经元不与本层的其他神经元交流</div><div><br clear="none"/></div><div>最后一层通常是classification layer (e.g. Softmax)</div><div><br clear="none"/></div><div> 除了第一层, 最后一层: </div><div>每层都有两个作用: 对于前一层作为隐藏层, 作为后一层的输入层</div><div><br clear="none"/></div><div>Generative<div><strong>Deep Autoencoders:</strong></div><div><br clear="none"/></div></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>由两个对称的Deep Brief Network组成:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [8].png" type="image/png" alt="Alt text"/><br clear="none"/></div><div><br clear="none"/></div><div>每层由Restricted Boltzmann Machine组成:</div><div><br clear="none"/></div><div>对于MNIST, 输入转化为binary</div><div><br clear="none"/></div><div>Encoding:</div><div><pre><code>784 (input) ----&gt; 1000 ----&gt; 500 ----&gt; 250 ----&gt; 100 -----&gt; 30<br clear="none"/><br clear="none"/></code>1000 &gt; 784, sigmoid-brief unit代表的信息量比实数少<br clear="none"/><br clear="none"/>Decoding:<br clear="none"/><br clear="none"/></pre><pre><code>784 (output) &lt;---- 1000 &lt;---- 500 &lt;---- 250 &lt;---- 30<br clear="none"/><br clear="none"/>用来降低维度, 图像搜索(压缩), 数据压缩, 信息检索<br clear="none"/><br clear="none"/><strong>scikit-learn nerualnetwork</strong>:<br clear="none"/><br clear="none"/>iris 数据库:<br clear="none"/><strong>https://en.wikipedia.org/wiki/Iris_flower_data_set</strong><br clear="none"/><br clear="none"/><a href="https://github.com/aigamedev/scikit-neuralnetwork" shape="rect">https://github.com/aigamedev/scikit-neuralnetwork</a><br clear="none"/><br clear="none"/>举例:<br clear="none"/><br clear="none"/>import logging<br clear="none"/>logging.basicConfig()<br clear="none"/><br clear="none"/>from sknn.mlp import Classifier, Layer<br clear="none"/><br clear="none"/>import numpy as np<br clear="none"/>from sklearn import cross_validation<br clear="none"/>from sklearn import datasets<br clear="none"/><br clear="none"/> <br clear="none"/>iris = datasets.load_iris()<br clear="none"/># iris.data.shape, iris.target.shape<br clear="none"/> <br clear="none"/>X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)<br clear="none"/> <br clear="none"/>nn = Classifier(<br clear="none"/> layers=[<br clear="none"/> Layer(&quot;Rectifier&quot;, units=100),<br clear="none"/> Layer(&quot;Linear&quot;)],<br clear="none"/> learning_rate=0.02,<br clear="none"/> n_iter=10)<br clear="none"/>nn.fit(X_train, y_train)<br clear="none"/> <br clear="none"/>y_pred = nn.predict(X_test)<br clear="none"/> <br clear="none"/>score = nn.score(X_test, y_test)<br clear="none"/><br clear="none"/># print(&quot;y_test&quot;, y_test)<br clear="none"/># print(&quot;y_pred&quot;, y_pred)<br clear="none"/>print(&quot;score&quot;, score)<br clear="none"/><br clear="none"/></code><br clear="none"/></pre></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="315"/>

<div>
<span><pre xml:space="preserve"><span>class</span> <span>FullyConnectedLayer</span><span>(</span><span>object</span><span>):</span>

    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>n_in</span><span>,</span> <span>n_out</span><span>,</span> <span>activation_fn</span><span>=</span><span>sigmoid</span><span>,</span> <span>p_dropout</span><span>=</span><span>0.0</span><span>):</span>
        <span>self</span><span>.</span><span>n_in</span> <span>=</span> <span>n_in</span>
        <span>self</span><span>.</span><span>n_out</span> <span>=</span> <span>n_out</span>
        <span>self</span><span>.</span><span>activation_fn</span> <span>=</span> <span>activation_fn</span>
        <span>self</span><span>.</span><span>p_dropout</span> <span>=</span> <span>p_dropout</span>
        <span># Initialize weights and biases</span>
        <span>self</span><span>.</span><span>w</span> <span>=</span> <span>theano</span><span>.</span><span>shared</span><span>(</span>
            <span>np</span><span>.</span><span>asarray</span><span>(</span>
                <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span>
                    <span>loc</span><span>=</span><span>0.0</span><span>,</span> <span>scale</span><span>=</span><span>np</span><span>.</span><span>sqrt</span><span>(</span><span>1.0</span><span>/</span><span>n_out</span><span>),</span> <span>size</span><span>=</span><span>(</span><span>n_in</span><span>,</span> <span>n_out</span><span>)),</span>
                <span>dtype</span><span>=</span><span>theano</span><span>.</span><span>config</span><span>.</span><span>floatX</span><span>),</span>
            <span>name</span><span>=</span><span>'w'</span><span>,</span> <span>borrow</span><span>=</span><span>True</span><span>)</span>
        <span>self</span><span>.</span><span>b</span> <span>=</span> <span>theano</span><span>.</span><span>shared</span><span>(</span>
            <span>np</span><span>.</span><span>asarray</span><span>(</span><span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>loc</span><span>=</span><span>0.0</span><span>,</span> <span>scale</span><span>=</span><span>1.0</span><span>,</span> <span>size</span><span>=</span><span>(</span><span>n_out</span><span>,)),</span>
                       <span>dtype</span><span>=</span><span>theano</span><span>.</span><span>config</span><span>.</span><span>floatX</span><span>),</span>
            <span>name</span><span>=</span><span>'b'</span><span>,</span> <span>borrow</span><span>=</span><span>True</span><span>)</span>
        <span>self</span><span>.</span><span>params</span> <span>=</span> <span>[</span><span>self</span><span>.</span><span>w</span><span>,</span> <span>self</span><span>.</span><span>b</span><span>]</span>

    <span>def</span> <span>set_inpt</span><span>(</span><span>self</span><span>,</span> <span>inpt</span><span>,</span> <span>inpt_dropout</span><span>,</span> <span>mini_batch_size</span><span>):</span>
        <span>self</span><span>.</span><span>inpt</span> <span>=</span> <span>inpt</span><span>.</span><span>reshape</span><span>((</span><span>mini_batch_size</span><span>,</span> <span>self</span><span>.</span><span>n_in</span><span>))</span>
        <span>self</span><span>.</span><span>output</span> <span>=</span> <span>self</span><span>.</span><span>activation_fn</span><span>(</span>
            <span>(</span><span>1</span><span>-</span><span>self</span><span>.</span><span>p_dropout</span><span>)</span><span>*</span><span>T</span><span>.</span><span>dot</span><span>(</span><span>self</span><span>.</span><span>inpt</span><span>,</span> <span>self</span><span>.</span><span>w</span><span>)</span> <span>+</span> <span>self</span><span>.</span><span>b</span><span>)</span>
        <span>self</span><span>.</span><span>y_out</span> <span>=</span> <span>T</span><span>.</span><span>argmax</span><span>(</span><span>self</span><span>.</span><span>output</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
        <span>self</span><span>.</span><span>inpt_dropout</span> <span>=</span> <span>dropout_layer</span><span>(</span>
            <span>inpt_dropout</span><span>.</span><span>reshape</span><span>((</span><span>mini_batch_size</span><span>,</span> <span>self</span><span>.</span><span>n_in</span><span>)),</span> <span>self</span><span>.</span><span>p_dropout</span><span>)</span>
        <span>self</span><span>.</span><span>output_dropout</span> <span>=</span> <span>self</span><span>.</span><span>activation_fn</span><span>(</span>
            <span>T</span><span>.</span><span>dot</span><span>(</span><span>self</span><span>.</span><span>inpt_dropout</span><span>,</span> <span>self</span><span>.</span><span>w</span><span>)</span> <span>+</span> <span>self</span><span>.</span><span>b</span><span>)</span>

    <span>def</span> <span>accuracy</span><span>(</span><span>self</span><span>,</span> <span>y</span><span>):</span>
        <span>&quot;Return the accuracy for the mini-batch.&quot;</span>
        <span>return</span> <span>T</span><span>.</span><span>mean</span><span>(</span><span>T</span><span>.</span><span>eq</span><span>(</span><span>y</span><span>,</span> <span>self</span><span>.</span><span>y_out</span><span>))<br clear="none"/><br clear="none"/>Theano: shared variables, 可以用gpu计算<br clear="none"/>set_inpt: 正向更新神经网络<br clear="none"/></span>用dropout和不用dropout两个版本<br clear="none"/><br clear="none"/></pre><div><pre xml:space="preserve"><span>class</span> <span>Network</span><span>(</span><span>object</span><span>):</span>
    
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>layers</span><span>,</span> <span>mini_batch_size</span><span>):</span>
        <span>&quot;&quot;&quot;Takes a list of `layers`, describing the network architecture, and</span>
<span>        a value for the `mini_batch_size` to be used during training</span>
<span>        by stochastic gradient descent.</span>

<span>        &quot;&quot;&quot;</span>
        <span>self</span><span>.</span><span>layers</span> <span>=</span> <span>layers</span>
        <span>self</span><span>.</span><span>mini_batch_size</span> <span>=</span> <span>mini_batch_size</span>
        <span>self</span><span>.</span><span>params</span> <span>=</span> <span>[</span><span>param</span> <span>for</span> <span>layer</span> <span>in</span> <span>self</span><span>.</span><span>layers</span> <span>for</span> <span>param</span> <span>in</span> <span>layer</span><span>.</span><span>params</span><span>]</span>
        <span>self</span><span>.</span><span>x</span> <span>=</span> <span>T</span><span>.</span><span>matrix</span><span>(</span><span>&quot;x&quot;</span><span>)</span>  
        <span>self</span><span>.</span><span>y</span> <span>=</span> <span>T</span><span>.</span><span>ivector</span><span>(</span><span>&quot;y&quot;</span><span>)</span>
        <span>init_layer</span> <span>=</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>0</span><span>]</span>
        <span>init_layer</span><span>.</span><span>set_inpt</span><span>(</span><span>self</span><span>.</span><span>x</span><span>,</span> <span>self</span><span>.</span><span>x</span><span>,</span> <span>self</span><span>.</span><span>mini_batch_size</span><span>)</span>
        <span>for</span> <span>j</span> <span>in</span> <span>xrange</span><span>(</span><span>1</span><span>,</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>layers</span><span>)):</span>
            <span>prev_layer</span><span>,</span> <span>layer</span>  <span>=</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>j</span><span>-</span><span>1</span><span>],</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>j</span><span>]</span>
            <span>layer</span><span>.</span><span>set_inpt</span><span>(</span>
                <span>prev_layer</span><span>.</span><span>output</span><span>,</span> <span>prev_layer</span><span>.</span><span>output_dropout</span><span>,</span> <span>self</span><span>.</span><span>mini_batch_size</span><span>)</span>
        <span>self</span><span>.</span><span>output</span> <span>=</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>output</span>
        <span>self</span><span>.</span><span>output_dropout</span> <span>=</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>output_dropout</span>
</pre><div><span><br clear="none"/></span></div></div><p> </p><p><span>self.x = T.matrix(&quot;x&quot;)</span></p><p><span>self.y = T.ivector(&quot;y&quot;)<br clear="none"/><br clear="none"/></span></p><pre xml:space="preserve"><span>param</span> <span>for</span> <span>layer</span> <span>in</span> <span>self</span><span>.</span><span>layers</span> <span>for</span> <span>param</span> <span>in</span> <span>layer</span><span>.</span><span>params<br clear="none"/></span>把每层的参数全部打包放到一个list里面</pre><p><span><br clear="none"/></span></p><p>定义 Theano里面的symbolic 变量<br clear="none"/>Theano: 用的是数学里面的变量表达, 并不是变量的值</p><p>Theano: 方程实现了各种基本的数学运算</p><p><br clear="none"/></p><div><pre xml:space="preserve">        <span>init_layer</span><span>.</span><span>set_inpt</span><span>(</span><span>self</span><span>.</span><span>x</span><span>,</span> <span>self</span><span>.</span><span>x</span><span>,</span> <span>self</span><span>.</span><span>mini_batch_size</span><span>)</span>
</pre><div><span><br clear="none"/></span></div></div><p> </p><p>输入是每次一个mini-batch</p><p>传入self.x两次, 因为用或者不用dropout</p><p>for循环向前传输神经网络中的x</p><div><pre xml:space="preserve">  <span>def</span> <span>SGD</span><span>(</span><span>self</span><span>,</span> <span>training_data</span><span>,</span> <span>epochs</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>eta</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>,</span> <span>lmbda</span><span>=</span><span>0.0</span><span>):</span>
        <span>&quot;&quot;&quot;Train the network using mini-batch stochastic gradient descent.&quot;&quot;&quot;</span>
        <span>training_x</span><span>,</span> <span>training_y</span> <span>=</span> <span>training_data</span>
        <span>validation_x</span><span>,</span> <span>validation_y</span> <span>=</span> <span>validation_data</span>
        <span>test_x</span><span>,</span> <span>test_y</span> <span>=</span> <span>test_data</span>

        <span># compute number of minibatches for training, validation and testing</span>
        <span>num_training_batches</span> <span>=</span> <span>size</span><span>(</span><span>training_data</span><span>)</span><span>/</span><span>mini_batch_size</span>
        <span>num_validation_batches</span> <span>=</span> <span>size</span><span>(</span><span>validation_data</span><span>)</span><span>/</span><span>mini_batch_size</span>
        <span>num_test_batches</span> <span>=</span> <span>size</span><span>(</span><span>test_data</span><span>)</span><span>/</span><span>mini_batch_size</span>

        <span># define the (regularized) cost function, symbolic gradients, and updates</span>
        <span>l2_norm_squared</span> <span>=</span> <span>sum</span><span>([(</span><span>layer</span><span>.</span><span>w</span><span>**</span><span>2</span><span>)</span><span>.</span><span>sum</span><span>()</span> <span>for</span> <span>layer</span> <span>in</span> <span>self</span><span>.</span><span>layers</span><span>])</span>
        <span>cost</span> <span>=</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>cost</span><span>(</span><span>self</span><span>)</span><span>+</span>\
               <span>0.5</span><span>*</span><span>lmbda</span><span>*</span><span>l2_norm_squared</span><span>/</span><span>num_training_batches</span>
        <span>grads</span> <span>=</span> <span>T</span><span>.</span><span>grad</span><span>(</span><span>cost</span><span>,</span> <span>self</span><span>.</span><span>params</span><span>)</span>
        <span>updates</span> <span>=</span> <span>[(</span><span>param</span><span>,</span> <span>param</span><span>-</span><span>eta</span><span>*</span><span>grad</span><span>)</span> 
                   <span>for</span> <span>param</span><span>,</span> <span>grad</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>params</span><span>,</span> <span>grads</span><span>)]</span>

        <span># define functions to train a mini-batch, and to compute the</span>
        <span># accuracy in validation and test mini-batches.</span>
        <span>i</span> <span>=</span> <span>T</span><span>.</span><span>lscalar</span><span>()</span> <span># mini-batch index</span>
        <span>train_mb</span> <span>=</span> <span>theano</span><span>.</span><span>function</span><span>(</span>
            <span>[</span><span>i</span><span>],</span> <span>cost</span><span>,</span> <span>updates</span><span>=</span><span>updates</span><span>,</span>
            <span>givens</span><span>=</span><span>{</span>
                <span>self</span><span>.</span><span>x</span><span>:</span>
                <span>training_x</span><span>[</span><span>i</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>:</span> <span>(</span><span>i</span><span>+</span><span>1</span><span>)</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>],</span>
                <span>self</span><span>.</span><span>y</span><span>:</span> 
                <span>training_y</span><span>[</span><span>i</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>:</span> <span>(</span><span>i</span><span>+</span><span>1</span><span>)</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>]</span>
            <span>})</span>
        <span>validate_mb_accuracy</span> <span>=</span> <span>theano</span><span>.</span><span>function</span><span>(</span>
            <span>[</span><span>i</span><span>],</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>accuracy</span><span>(</span><span>self</span><span>.</span><span>y</span><span>),</span>
            <span>givens</span><span>=</span><span>{</span>
                <span>self</span><span>.</span><span>x</span><span>:</span> 
                <span>validation_x</span><span>[</span><span>i</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>:</span> <span>(</span><span>i</span><span>+</span><span>1</span><span>)</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>],</span>
                <span>self</span><span>.</span><span>y</span><span>:</span> 
                <span>validation_y</span><span>[</span><span>i</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>:</span> <span>(</span><span>i</span><span>+</span><span>1</span><span>)</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>]</span>
            <span>})</span>
        <span>test_mb_accuracy</span> <span>=</span> <span>theano</span><span>.</span><span>function</span><span>(</span>
            <span>[</span><span>i</span><span>],</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>accuracy</span><span>(</span><span>self</span><span>.</span><span>y</span><span>),</span>
            <span>givens</span><span>=</span><span>{</span>
                <span>self</span><span>.</span><span>x</span><span>:</span> 
                <span>test_x</span><span>[</span><span>i</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>:</span> <span>(</span><span>i</span><span>+</span><span>1</span><span>)</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>],</span>
                <span>self</span><span>.</span><span>y</span><span>:</span> 
                <span>test_y</span><span>[</span><span>i</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>:</span> <span>(</span><span>i</span><span>+</span><span>1</span><span>)</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>]</span>
            <span>})</span>
        <span>self</span><span>.</span><span>test_mb_predictions</span> <span>=</span> <span>theano</span><span>.</span><span>function</span><span>(</span>
            <span>[</span><span>i</span><span>],</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>y_out</span><span>,</span>
            <span>givens</span><span>=</span><span>{</span>
                <span>self</span><span>.</span><span>x</span><span>:</span> 
                <span>test_x</span><span>[</span><span>i</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>:</span> <span>(</span><span>i</span><span>+</span><span>1</span><span>)</span><span>*</span><span>self</span><span>.</span><span>mini_batch_size</span><span>]</span>
            <span>})</span>
        <span># Do the actual training</span>
        <span>best_validation_accuracy</span> <span>=</span> <span>0.0</span>
        <span>for</span> <span>epoch</span> <span>in</span> <span>xrange</span><span>(</span><span>epochs</span><span>):</span>
            <span>for</span> <span>minibatch_index</span> <span>in</span> <span>xrange</span><span>(</span><span>num_training_batches</span><span>):</span>
                <span>iteration</span> <span>=</span> <span>num_training_batches</span><span>*</span><span>epoch</span><span>+</span><span>minibatch_index</span>
                <span>if</span> <span>iteration</span> 
                    <span>print</span><span>(</span><span>&quot;Training mini-batch number {0}&quot;</span><span>.</span><span>format</span><span>(</span><span>iteration</span><span>))</span>
                <span>cost_ij</span> <span>=</span> <span>train_mb</span><span>(</span><span>minibatch_index</span><span>)</span>
                <span>if</span> <span>(</span><span>iteration</span><span>+</span><span>1</span><span>)</span> 
                    <span>validation_accuracy</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
                        <span>[</span><span>validate_mb_accuracy</span><span>(</span><span>j</span><span>)</span> <span>for</span> <span>j</span> <span>in</span> <span>xrange</span><span>(</span><span>num_validation_batches</span><span>)])</span>
                    <span>print</span><span>(</span><span>&quot;Epoch {0}: validation accuracy {1:.2</span>
                        <span>epoch</span><span>,</span> <span>validation_accuracy</span><span>))</span>
                    <span>if</span> <span>validation_accuracy</span> <span>&gt;=</span> <span>best_validation_accuracy</span><span>:</span>
                        <span>print</span><span>(</span><span>&quot;This is the best validation accuracy to date.&quot;</span><span>)</span>
                        <span>best_validation_accuracy</span> <span>=</span> <span>validation_accuracy</span>
                        <span>best_iteration</span> <span>=</span> <span>iteration</span>
                        <span>if</span> <span>test_data</span><span>:</span>
                            <span>test_accuracy</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
                                <span>[</span><span>test_mb_accuracy</span><span>(</span><span>j</span><span>)</span> <span>for</span> <span>j</span> <span>in</span> <span>xrange</span><span>(</span><span>num_test_batches</span><span>)])</span>
                            <span>print</span><span>(</span><span>'The corresponding test accuracy is {0:.2</span>
                                <span>test_accuracy</span><span>))</span>
        <span>print</span><span>(</span><span>&quot;Finished training network.&quot;</span><span>)</span>
        <span>print</span><span>(</span><span>&quot;Best validation accuracy of {0:.2</span>
            <span>best_validation_accuracy</span><span>,</span> <span>best_iteration</span><span>))</span>
        <span>print</span><span>(</span><span>&quot;Corresponding test accuracy of {0:.2</span>
</pre><div><span><br clear="none"/></span></div></div><p> </p><p><br clear="none"/></p><pre xml:space="preserve">        <span># define the (regularized) cost function, symbolic gradients, and updates</span>
        <span>l2_norm_squared</span> <span>=</span> <span>sum</span><span>([(</span><span>layer</span><span>.</span><span>w</span><span>**</span><span>2</span><span>)</span><span>.</span><span>sum</span><span>()</span> <span>for</span> <span>layer</span> <span>in</span> <span>self</span><span>.</span><span>layers</span><span>])</span>
        <span>cost</span> <span>=</span> <span>self</span><span>.</span><span>layers</span><span>[</span><span>-</span><span>1</span><span>]</span><span>.</span><span>cost</span><span>(</span><span>self</span><span>)</span><span>+</span>\
               <span>0.5</span><span>*</span><span>lmbda</span><span>*</span><span>l2_norm_squared</span><span>/</span><span>num_training_batches</span>
        <span>grads</span> <span>=</span> <span>T</span><span>.</span><span>grad</span><span>(</span><span>cost</span><span>,</span> <span>self</span><span>.</span><span>params</span><span>)</span>
        <span>updates</span> <span>=</span> <span>[(</span><span>param</span><span>,</span> <span>param</span><span>-</span><span>eta</span><span>*</span><span>grad</span><span>)</span> 
                   <span>for</span> <span>param</span><span>,</span> <span>grad</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>params</span><span>,</span> <span>grads</span><span>)]\<br clear="none"/><br clear="none"/>第一步用symbolic variable来计算regularized log-likelihood cost<br clear="none"/><br clear="none"/></span></pre><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><span><br clear="none"/></span></p></span>
</div>
<hr>
<a name="316"/>

<div>
<span><div>Feature Maps:</div><div><br clear="none"/></div><div>20个feature maps</div><div><br clear="none"/></div><div><pre xml:space="preserve"><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [9].png" type="image/png" width="550px"/></pre></div><div><br clear="none"/></div><div>基准:</div><div><br clear="none"/></div><div>3层</div><div>隐藏层: 100个神经元</div><div>训练60个epochs</div><div>学习率 = 0.1</div><div>mini-batch size: 10</div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>network3</span>
<span>&gt;&gt;&gt;</span> <span>from</span> <span>network3</span> <span>import</span> <span>Network</span>
<span>&gt;&gt;&gt;</span> <span>from</span> <span>network3</span> <span>import</span> <span>ConvPoolLayer</span><span>,</span> <span>FullyConnectedLayer</span><span>,</span> <span>SoftmaxLayer</span>
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> <span>network3</span><span>.</span><span>load_data_shared</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>mini_batch_size</span> <span>=</span> <span>10</span>
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>Network</span><span>([</span>
        <span>FullyConnectedLayer</span><span>(</span><span>n_in</span><span>=</span><span>784</span><span>,</span> <span>n_out</span><span>=</span><span>100</span><span>),</span>
        <span>SoftmaxLayer</span><span>(</span><span>n_in</span><span>=</span><span>100</span><span>,</span> <span>n_out</span><span>=</span><span>10</span><span>)],</span> <span>mini_batch_size</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>60</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>0.1</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>)<br clear="none"/><br clear="none"/></span>结果: 97.8 accuracy  (上节课98.04%<br clear="none"/>这次: 没有regularization, 上次有<br clear="none"/>这次: softmax 上次: sigmoid + cross-entropy<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/>加入convolution层:<br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [10].png" type="image/png" width="550px"/><br clear="none"/></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>Network</span><span>([</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>20</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>)),</span>
        <span>FullyConnectedLayer</span><span>(</span><span>n_in</span><span>=</span><span>20</span><span>*</span><span>12</span><span>*</span><span>12</span><span>,</span> <span>n_out</span><span>=</span><span>100</span><span>),</span>
        <span>SoftmaxLayer</span><span>(</span><span>n_in</span><span>=</span><span>100</span><span>,</span> <span>n_out</span><span>=</span><span>10</span><span>)],</span> <span>mini_batch_size</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>60</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>0.1</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>)</span> <br clear="none"/><br clear="none"/>准确率: 98.78 比上次有显著提高<br clear="none"/><br clear="none"/><br clear="none"/>再加入一层convolution (共两层):<br clear="none"/></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>Network</span><span>([</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>20</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>)),</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>20</span><span>,</span> <span>12</span><span>,</span> <span>12</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>40</span><span>,</span> <span>20</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>)),</span>
        <span>FullyConnectedLayer</span><span>(</span><span>n_in</span><span>=</span><span>40</span><span>*</span><span>4</span><span>*</span><span>4</span><span>,</span> <span>n_out</span><span>=</span><span>100</span><span>),</span>
        <span>SoftmaxLayer</span><span>(</span><span>n_in</span><span>=</span><span>100</span><span>,</span> <span>n_out</span><span>=</span><span>10</span><span>)],</span> <span>mini_batch_size</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>60</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>0.1</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>)</span>        <br clear="none"/><br clear="none"/>准确率: 99.06% (再一次刷新)<br clear="none"/><br clear="none"/>用Rectified Linear Units代替sigmoid:<br clear="none"/>f(z) = max(0, z)<br clear="none"/><br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>Network</span><span>([</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>20</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>20</span><span>,</span> <span>12</span><span>,</span> <span>12</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>40</span><span>,</span> <span>20</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>FullyConnectedLayer</span><span>(</span><span>n_in</span><span>=</span><span>40</span><span>*</span><span>4</span><span>*</span><span>4</span><span>,</span> <span>n_out</span><span>=</span><span>100</span><span>,</span> <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>SoftmaxLayer</span><span>(</span><span>n_in</span><span>=</span><span>100</span><span>,</span> <span>n_out</span><span>=</span><span>10</span><span>)],</span> <span>mini_batch_size</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>60</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>0.03</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>,</span> <span>lmbda</span><span>=</span><span>0.1</span><span>)<br clear="none"/><br clear="none"/></span>准确率: 99.23 比之前用sigmoid函数的99.06%稍有提高<br clear="none"/><br clear="none"/>库大训练集: 每个图像向上,下,左,右移动一个像素<br clear="none"/>总训练集: 50,000 * 5 = 250,000<br clear="none"/><br clear="none"/></pre><div><pre xml:space="preserve"><span>$ </span>python expand_mnist.py
</pre><div><br clear="none"/></div></div><p> </p><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>expanded_training_data</span><span>,</span> <span>_</span><span>,</span> <span>_</span> <span>=</span> <span>network3</span><span>.</span><span>load_data_shared</span><span>(</span>
        <span>&quot;../data/mnist_expanded.pkl.gz&quot;</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>Network</span><span>([</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>20</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>20</span><span>,</span> <span>12</span><span>,</span> <span>12</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>40</span><span>,</span> <span>20</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>FullyConnectedLayer</span><span>(</span><span>n_in</span><span>=</span><span>40</span><span>*</span><span>4</span><span>*</span><span>4</span><span>,</span> <span>n_out</span><span>=</span><span>100</span><span>,</span> <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>SoftmaxLayer</span><span>(</span><span>n_in</span><span>=</span><span>100</span><span>,</span> <span>n_out</span><span>=</span><span>10</span><span>)],</span> <span>mini_batch_size</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>expanded_training_data</span><span>,</span> <span>60</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>0.03</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>,</span> <span>lmbda</span><span>=</span><span>0.1</span><span>)</span>
</pre><div><span><br clear="none"/></span></div></div><p> </p><p> 结果: 99.37%</p><p><br clear="none"/></p><p>加入一个100个神经元的隐藏层在fully-connected层:</p><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>Network</span><span>([</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>20</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>20</span><span>,</span> <span>12</span><span>,</span> <span>12</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>40</span><span>,</span> <span>20</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>FullyConnectedLayer</span><span>(</span><span>n_in</span><span>=</span><span>40</span><span>*</span><span>4</span><span>*</span><span>4</span><span>,</span> <span>n_out</span><span>=</span><span>100</span><span>,</span> <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>FullyConnectedLayer</span><span>(</span><span>n_in</span><span>=</span><span>100</span><span>,</span> <span>n_out</span><span>=</span><span>100</span><span>,</span> <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>SoftmaxLayer</span><span>(</span><span>n_in</span><span>=</span><span>100</span><span>,</span> <span>n_out</span><span>=</span><span>10</span><span>)],</span> <span>mini_batch_size</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>expanded_training_data</span><span>,</span> <span>60</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>0.03</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>,</span> <span>lmbda</span><span>=</span><span>0.1</span><span>)<br clear="none"/><br clear="none"/><br clear="none"/></span>结果: 99.43%, 并没有大的提高</pre><p>有可能overfit</p><p>加上dropout到最后一个fully-connected层:</p><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>expanded_training_data</span><span>,</span> <span>_</span><span>,</span> <span>_</span> <span>=</span> <span>network3</span><span>.</span><span>load_data_shared</span><span>(</span>
        <span>&quot;../data/mnist_expanded.pkl.gz&quot;</span><span>)</span></pre><pre xml:space="preserve"><span><br clear="none"/>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>Network</span><span>([</span></pre><pre xml:space="preserve">        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>20</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>ConvPoolLayer</span><span>(</span><span>image_shape</span><span>=</span><span>(</span><span>mini_batch_size</span><span>,</span> <span>20</span><span>,</span> <span>12</span><span>,</span> <span>12</span><span>),</span> 
                      <span>filter_shape</span><span>=</span><span>(</span><span>40</span><span>,</span> <span>20</span><span>,</span> <span>5</span><span>,</span> <span>5</span><span>),</span> 
                      <span>poolsize</span><span>=</span><span>(</span><span>2</span><span>,</span> <span>2</span><span>),</span> 
                      <span>activation_fn</span><span>=</span><span>ReLU</span><span>),</span>
        <span>FullyConnectedLayer</span><span>(</span>
            <span>n_in</span><span>=</span><span>40</span><span>*</span><span>4</span><span>*</span><span>4</span><span>,</span> <span>n_out</span><span>=</span><span>1000</span><span>,</span> <span>activation_fn</span><span>=</span><span>ReLU</span><span>,</span> <span>p_dropout</span><span>=</span><span>0.5</span><span>),</span>
        <span>FullyConnectedLayer</span><span>(</span>
            <span>n_in</span><span>=</span><span>1000</span><span>,</span> <span>n_out</span><span>=</span><span>1000</span><span>,</span> <span>activation_fn</span><span>=</span><span>ReLU</span><span>,</span> <span>p_dropout</span><span>=</span><span>0.5</span><span>),</span>
        <span>SoftmaxLayer</span><span>(</span><span>n_in</span><span>=</span><span>1000</span><span>,</span> <span>n_out</span><span>=</span><span>10</span><span>,</span> <span>p_dropout</span><span>=</span><span>0.5</span><span>)],</span> 
        <span>mini_batch_size</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>expanded_training_data</span><span>,</span> <span>40</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>0.03</span><span>,</span> 
            <span>validation_data</span><span>,</span> <span>test_data</span><span>)<br clear="none"/><br clear="none"/><br clear="none"/></span>结果: 99.60% 显著提高<br clear="none"/><br clear="none"/>epochs: 减少到了40<br clear="none"/>隐藏层有 1000 个神经元<br clear="none"/><br clear="none"/>Ensemble of network: 训练多个神经网络, 投票决定结果, 有时会提高<br clear="none"/><br clear="none"/>误识别的图像:<br clear="none"/><br clear="none"/><br clear="none"/></pre><p><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [11].png" type="image/png" width="580px"/></p><p><br clear="none"/></p><p>为何只对最后一层用dropout?</p><p><br clear="none"/></p><p>CNN本身的convolution层对于overfitting有防止作用: 共享的权重造成convolution filter强迫对于整个图像进行学习</p><p><br clear="none"/></p><p>为什么可以克服深度学习里面的一些困难?</p><p><br clear="none"/></p><p>用CNN大大减少了参数数量</p><p>用dropout减少了overfitting</p><p>用Rectified Linear Units代替了sigmoid, 避免了overfitting, 不同层学习率差别大的问题</p><p>用GPU计算更快, 每次更新较少, 但是可以训练很多次</p><p><br clear="none"/></p><p>目前的深度神经网络有多深? (多少层)?</p><p>最多有20多层</p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><p><br clear="none"/></p><pre xml:space="preserve"><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><br clear="none"/><br clear="none"/></pre></div></span>
</div>
<hr>
<a name="322"/>

<div>
<span><div>目前总体来讲最流行, 表现最好的算法:</div><div>Convolution Neural Network (CNN)</div><div><br clear="none"/></div><div>MNIST</div><div><br clear="none"/></div><div>CNN, gpu, deep network, dropout, ensembles</div><div><br clear="none"/></div><div>结果达到接近人肉眼识别水平:</div><div><br clear="none"/></div><div>9,967 / 10,000 识别正确</div><div><br clear="none"/></div><div>以下是误识别的图片 </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [12].png" type="image/png" width="580px"/><br clear="none"/></div><div> 其中很多对于人肉眼都不容易识别</div><div><br clear="none"/></div><div>之前的神经网络</div><div><br clear="none"/></div><div><br clear="none"/></div><div>相邻层之前所有的神经元都两两相连<br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-25 20-24-50.png" type="image/png"/></div><div><br clear="none"/></div><div>输入层: 图像像素值</div><div>输出层: 0-9<br clear="none"/><br clear="none"/></div><div>CNN结构很不一样, 输入是一个二维的神经元 (28x28):</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [13].png" type="image/png"/><br clear="none"/></div><div><br clear="none"/></div><div><div>Local receptive fields:</div><div><br clear="none"/></div></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [14].png" type="image/png"/></div><div><br clear="none"/></div><div>5x5</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [15].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [16].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>28x25,    5x5                                     =&gt; 24x24</div><div><br clear="none"/></div><div>stride: 每次移动多少</div><div><br clear="none"/></div><div>共享权重和偏向(shared weights and biases):</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-25 20-32-47.png" type="image/png"/></div><div>w: 5x5</div><div><br clear="none"/></div><div>对于第一个隐藏层, 所有神经元探测到同样的特征, 只是根据不同位置</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [17].png" type="image/png"/></div><div><br clear="none"/></div><div>保留图像原始的形状特征</div><div><br clear="none"/></div><div>Feature map:  从输入层转化到输出层</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [18].png" type="image/png"/></div><div><br clear="none"/></div><div>以上3个feature maps, 每个是5x5</div><div><br clear="none"/></div><div>通常一些表现较好的方法都使用更多的feature map:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [19].png" type="image/png" width="400px"/></div><div><br clear="none"/></div><div>以上是学习出的, 根据5x5的feature map</div><div><br clear="none"/></div><div>浅色代表更小的权重(负数)</div><div><br clear="none"/></div><div>表明CNN在学习</div><div><br clear="none"/></div><div><br clear="none"/></div><div>共享的权重和偏向(weights, bias)大大减少了参数的数量:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [20].png" type="image/png"/></div><div><br clear="none"/></div><div>对于每一个feature map, 需要 5x5=25个权重参数, 加上1个偏向b, 26个</div><div>如果有20个feature maps, 总共26x20=520个参数就可以定义CNN</div><div><br clear="none"/>如果像之前的神经网络, 两两相连, 需要 28x28 = 784 输入层, 加上第一个隐藏层30个神经元, 则需要784x30再加上30个b, 总共23,550个参数! 多了40倍的参数.</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-25 20-32-47 [1].png" type="image/png"/></div><div>也可以写成:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-25 20-55-44.png" type="image/png"/><br clear="none"/></div><div>Pooling layers:</div><div><br clear="none"/></div><div>浓缩神经网聚的代表性, 减小尺寸:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [21].png" type="image/png"/></div><div><br clear="none"/></div><div>24x24 ,    2x2 pooling =&gt;                                          12x12</div><div><br clear="none"/></div><div><br clear="none"/></div><div>多个feature maps:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [22].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>重要特征点找到之后, 绝对位置并不重要, 相对位置更加重要</div><div><br clear="none"/></div><div>其他pooling: L2 pooling, 平方和开方</div><div><br clear="none"/></div><div><br clear="none"/></div><div>以上所有步骤结合在一起:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [23].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>还是用Backpropagation, gradient descent解决</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="336"/>

<div>
<span><div><strong><br clear="none"/></strong></div><div><strong><br clear="none"/></strong></div><div><strong><img src="https://www.evernote.com/shard/s27/res/c9fdb1e8-4d21-4d71-989d-dee5c8ba6752/screenshot-neuralnetworksanddeeplearning.com%202015-09-24%2021-17-48.png"></img></strong></div><div><br clear="none"/></div><div><strong>Exploding gradient problem:</strong></div><div>加入想修正以上问题:</div><div>1. 初始化比较大的权重: 比如 w1=w2=w3=w4=100</div><div>2. 初始化b使</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-22-47.png" type="image/png"/></div><div>不要太小</div><div>比如为了让σ′最大(也就是=1/4), 我们可以通过调节b让z=0:</div><div>b1 = -100*a0</div><div>z1 = 100 * a0 + -100*a0 = 0</div><div><br clear="none"/></div><div>这种情况下:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-27-25.png" type="image/png"/></div><div>= 100 * 1/4 = 25</div><div><br clear="none"/></div><div>每层是前一层的25倍, 又出现了exploding的问题</div><div><br clear="none"/></div><div><br clear="none"/></div><div>从根本来讲, 不是vanishing或者exploding的问题, 而是后面层的的梯度是前面层的累积的乘积, 所以神经网络非常不稳定. 唯一可能的情况是以上的连续乘积刚好平衡大约等于1, 但是这种几率非常小.</div><div><br clear="none"/></div><div>所以, 这是一个不稳定的梯度问题, 通常有多层后, 每层网络都以非常不同的速率学习</div><div><br clear="none"/></div><div>总体, vanishing problem具有普遍性:</div><div><br clear="none"/></div><div>如果想要客克服vanishing problem, 需要</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-27-25 [1].png" type="image/png"/></div><div>的绝对值&gt;1, 我们可以尝试赋值w很大, 但是问题是 σ′(z) 也取决于w: σ′(z)=σ′(wa+b)</div><div>所以我们要让w大的时候, 还得注意不能让σ′(wa+b)变小, 这种情况非常少见, 除非输入值在一个非常小的区间内</div><div><br clear="none"/></div><div>刚才的例子只是一每层一个神经元:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-49-35.png" type="image/png"/></div><div><br clear="none"/></div><div>在每层多个神经元的情况下:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-40-02.png" type="image/png"/></div><div>在l层的gradient (L层神经网络):</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-41-15.png" type="image/png"/></div><div>矩阵和向量的表示, 与之前类似</div><div><br clear="none"/></div><div>所以只要是sigmoid函数的神经网络都会造成gradient更新的时候及其不稳定, vanishing or exploding问题</div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>训练深度神经网络的其他难点:</strong></div><div><br clear="none"/></div><div> 2010 Glorot and Bengio*: sigmoid函数造成输出层的activation大部分饱和0, 并且建议了其他的activation函数</div><div><br clear="none"/></div><div>2013 Sutskever, Martens, Dahl and Hinton*: 随机初始权重和偏向时, 提出momentum-based stochastic gradient descent</div><div><br clear="none"/></div><div><br clear="none"/></div><div>综上所属, 训练深度神经网络中有很多难点.</div><div>本节课: 神经网络的不稳定性</div><div>activation方程的选择</div><div>初始化权重和偏向的方法</div><div>具体更新的过程</div><div>hyper-parameter的选择</div><div><br clear="none"/></div><div>这些目前都是当前学术界研究的课题, 已经取得一些有效的解决方法</div><div><br clear="none"/></div><div><br clear="none"/></div><div>解决vanishing gradient方法:</div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-www.quora.com 2015-09-25 01-44-25.png" type="image/png"/></div><div>softplus函数可以被max函数模拟 max(0, x+N(0,1))</div><div><br clear="none"/></div><div>max函数叫做Rectified Linear Function (ReL)</div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-www.quora.com 2015-09-25 01-48-34.png" type="image/png"/></div><div>Sigmoid和ReL方程主要区别:</div><div><br clear="none"/></div><div>Sigmoid函数值在[0, 1], ReL函数值在[0, <span>∞], 所以sigmoid函数方面来描述概率, 而ReL适合用来描述实数</span></div><div><br clear="none"/></div><div>Sigmoid函数的gradient随着x增大或减小和消失</div><div>ReL 函数不会:</div><div>gradient = 0 (if x &lt; 0), gradient = 1 (x &gt; 0)</div><div><br clear="none"/></div><div>Rectified Linear Unit在神经网络中的优势:</div><div><br clear="none"/></div><div>不会产生vanishing gradient的问题</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="344"/>

<div>
<span><div><br clear="none"/></div><div>到目前为止, 我们例子中使用的神经网络一共只有3层 (一个隐藏层):</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-07-00.png" type="image/png"/></div><div>我们用以上神经网络达到了98%的accuracy</div><div><br clear="none"/></div><div>更深层的神经网络:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-08-45.png" type="image/png"/><br clear="none"/></div><div>可以学习到不同抽象程度的概念:</div><div>例如: 图像中: 第一层学到边角, 第二层学到一些基本形状, 第三层学到物体概念</div><div><br clear="none"/></div><div>如何训练深度神经网络?</div><div><br clear="none"/></div><div>难点: 神经网络的不同层学习的速率显著不同</div><div>          接近输出层学习速率比较合适时, 前面的层学习太慢, 有时被困住</div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>消失的gradient问题 (vanishing gradient problem):</strong></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span>
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()<br clear="none"/></span><br clear="none"/></pre></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>network2</span>
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>])<br clear="none"/></span><br clear="none"/></pre></div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.1</span><span>,</span> <span>lmbda</span><span>=</span><span>5.0</span><span>,</span> 
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>,</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/>结果: 96.48%<br clear="none"/><br clear="none"/><br clear="none"/>加入一个隐藏层:<br clear="none"/><br clear="none"/></span></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.1</span><span>,</span> <span>lmbda</span><span>=</span><span>5.0</span><span>,</span> 
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>,</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/></span>结果: 96.9%<br clear="none"/><br clear="none"/>再加入一个隐藏层:<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>30</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.1</span><span>,</span> <span>lmbda</span><span>=</span><span>5.0</span><span>,</span> 
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>,</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/><br clear="none"/></span>结果: 96.57%<br clear="none"/><br clear="none"/><br clear="none"/>为什么加入一层反而降低了准确率?<br clear="none"/><br clear="none"/>条形区域长度代表<span>∂</span><span>C</span><span><span><span>/</span></span></span><span>∂</span><span>b, Cost对于bias的变化率:</span></pre></div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-20-09.png" type="image/png"/></div><div><br clear="none"/></div><div>随机初始化, 看到第一层学习的速率远远低于第二层学习的速率</div><div><br clear="none"/></div><div>进一步通过计算来验证:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-32-16.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-34-03.png" type="image/png"/></div><div>代表学习速率</div><div>以上图中: <span><br clear="none"/>∥</span><span><span><span><span>δ</span></span><span><span>1</span></span></span></span><span>∥</span><span>=</span><span>0.07,  </span><span>∥</span><span><span><span><span>δ</span></span><span><span>2</span></span></span></span><span>∥</span><span>=</span><span>0.31</span></div><div><span><br clear="none"/></span></div><div>在5层的神经网络中: <span>[</span><span>784</span><span>,</span><span>30</span><span>,</span><span>30</span><span>,</span><span>30</span><span>,</span><span>10</span><span>]</span></div><div>学习速率分别为: <span>0.012, 0.060, and 0.283</span></div><div><br clear="none"/></div><div>以上只是初始的时候的学习率, 当神经网络在训练过程中, 随epoch增加时学习率变化:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [24].png" type="image/png" width="500px"/><br clear="none"/></div><div><br clear="none"/></div><div>1000张训练图片, 看出两层的学习率明显差异</div><div><br clear="none"/></div><div><br clear="none"/></div><div>另外一个例子: </div><div><span> </span><span><span><span><span><span><span>[</span><span>784</span><span>,</span><span>30</span><span>,</span><span>30</span><span>,</span><span>30</span><span>,</span><span>10</span><span>]</span></span></span></span></span></span></div><div><span><br clear="none"/></span></div><div><span><br clear="none"/></span></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [25].png" type="image/png" width="500px"/><br clear="none"/></div><div><br clear="none"/></div><div>再增加一层: <span><br clear="none"/>[</span><span>784</span><span>,</span><span>30</span><span>,</span><span>30</span><span>,</span><span>30</span><span>,</span><span>30</span><span>,</span><span>10</span><span>]</span></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [26].png" type="image/png" width="500px"/></div><div><br clear="none"/></div><div>可以看出, 第一个隐藏层比第四个几乎要慢100倍</div><div><br clear="none"/></div><div>这种现象普遍存在于神经网络之中, 叫做: vanishing gradient problem</div><div><br clear="none"/></div><div>另外一种情况是内层的梯度被外层大很多, 叫做exploding gradient problem</div><div><br clear="none"/></div><div>所以说神经网络算法用gradient之类的算法学习存在不稳定性</div><div><br clear="none"/></div><div>训练深度神经网络, 需要解决vanishing gradient problem</div><div><br clear="none"/></div><div><strong>造成vanishing gradient problem的原因:</strong></div><div><strong><br clear="none"/></strong></div><div>假设每层只有一个神经元</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-49-35 [1].png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-50-29.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-50-55.png" type="image/png"/></div><div>得到的<span>∂</span><span>C</span><span><span><span>/</span></span></span><span>∂</span><span><span><span><span>b</span></span><span><span>1的表达式:</span></span></span></span></div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-52-09.png" type="image/png"/></div><div><br clear="none"/></div><div>对b1的一个小变化引起C的变化</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-55-14.png" type="image/png"/><br clear="none"/></div><div>如何引起b1的变化:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-56-55.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 20-57-48.png" type="image/png"/><br clear="none"/></div><div>a1的变化又引起 z2 的变化: z2= w2*a1 + b2</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-01-20.png" type="image/png"/></div><div>把以上a1的变化代入z2的变化:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-02-45.png" type="image/png"/></div><div>以上公式的规律, 推z3, z4的变化, 一直到输出层, 得到:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-08-54.png" type="image/png"/></div><div>等式两边同除以b1的变化,得到:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-09-43.png" type="image/png"/><br clear="none"/></div><div><span><span><span><span><span><span>σ</span><span>′的图像:</span></span></span></span></span></span></div><div><span><span><span><span><span><span><br clear="none"/></span></span></span></span></span></span></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-11-48.png" type="image/png"/><br clear="none"/></div><div>函数最高点<span><span><span><span><span><span>σ</span><span>′(0) = 1/4</span></span></span></span></span></span></div><div><span><span><span><span><span><span><br clear="none"/></span></span></span></span></span></span></div><div><span><span><span><span><span><span>按照平时随机从正太分部(0,1)中随机产生权重的方法</span></span></span></span></span></span></div><div>大部分|w| &lt; 1,  </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-16-18.png" type="image/png"/></div><div>对于以上公式的多项乘积来讲, 层数越多, 连续乘积越小:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-24 21-17-48.png" type="image/png"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="391"/>

<div>
<span><div>我们到目前为止在神经网络中使用了好几个参数, hyper-parameters包括:</div><div>学习率(learning rate): <span>η</span></div><div>Regularization parameter: <span>λ</span></div><div><br clear="none"/></div><div>之前只是设置了一些合适的值, 如何来选择合适的hyper-parameters呢?</div><div><br clear="none"/></div><div>例如:</div><div>我们设置如下参数:</div><div>隐藏层: 30个神经元, mini-batch size: 10, 训练30个epochs</div><div><span>η</span><span>=</span><span>10.0, <span><span><span><span><span>λ</span><span>=</span><span>1000.0</span></span></span></span></span></span></div><div><span><br clear="none"/></span></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span>
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>import</span> <span>network2</span>
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>])      </span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>10.0</span><span>,</span> <span>lmbda</span> <span>=</span> <span>1000.0</span><span>,</span>
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>,</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/></span>结果:<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>Epoch</span> <span>0</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>1030</span> <span>/</span> <span>10000</span>

<span>Epoch</span> <span>1</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>990</span> <span>/</span> <span>10000</span>

<span>Epoch</span> <span>2</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>1009</span> <span>/</span> <span>10000</span>

<span>...</span>

<span>Epoch</span> <span>27</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>1009</span> <span>/</span> <span>10000</span>

<span>Epoch</span> <span>28</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>983</span> <span>/</span> <span>10000</span>

<span>Epoch</span> <span>29</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>967</span> <span>/</span> <span>10000<br clear="none"/><br clear="none"/><br clear="none"/></span>差到跟随机猜测一样!<br clear="none"/><br clear="none"/>神经网络中可变化调整的因素很多:<br clear="none"/><br clear="none"/>神经网络结构: 层数, 每层神经元个数多少<br clear="none"/>初始化w和b的方法<br clear="none"/>Cost函数<br clear="none"/>Regularization: L1, L2<br clear="none"/>Sigmoid输出还是Softmax?<br clear="none"/>使用Droput?<br clear="none"/>训练集大小<br clear="none"/>mini-batch size</pre><div>学习率(learning rate): <span>η</span></div><div>Regularization parameter: <span>λ</span></div><div><br clear="none"/></div><div><strong>总体策略: </strong></div><div><br clear="none"/></div><div>从简单的出发: 开始实验</div><div>如: MNIST数据集, 开始不知如何设置, 可以先简化使用0,1两类图, 减少80%数据量, 用两层神经网络[784, 2] (比[784, 30, 2]快)</div><div><br clear="none"/></div><div>更快的获取反馈: 之前每个epoch来检测准确率, 可以替换为每1000个图之后,</div><div>                           或者减少validation set的量, 比如用100代替10,000</div><div><br clear="none"/></div><div>重复实验:</div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>[:</span><span>1000</span><span>],</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>10.0</span><span>,</span> <span>lmbda</span> <span>=</span> <span>1000.0</span><span>,</span> \
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>[:</span><strong><span>100</span></strong><span>],</span> \
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)</span>
<span>Epoch</span> <span>0</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>10</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>1</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>10</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>2</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>10</span> <span>/</span> <span>100</span>
<span>...<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/></span>更快得到反馈, 之前可能每轮要等10秒,现在不到1秒:<br clear="none"/><br clear="none"/><br clear="none"/><span>λ之前设置为1000, </span>因为减少了训练集的数量, <span>λ为了保证weight decay一样,对应的减少<span>λ = 20.0<br clear="none"/><br clear="none"/></span></span></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>[:</span><span>1000</span><span>],</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>10.0</span><span>,</span> <span>lmbda</span> <span>=</span> <span>20.0</span><span>,</span> \
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>[:</span><span>100</span><span>],</span> \
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)</span></pre><pre xml:space="preserve"><span><span><br clear="none"/><br clear="none"/></span></span>结果:<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>Epoch</span> <span>0</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>12</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>1</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>14</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>2</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>25</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>3</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>18</span> <span>/</span> <span>100<br clear="none"/><br clear="none"/></span><br clear="none"/></pre><pre xml:space="preserve">也许学习率<span>η</span><span>=</span><span>10.0太低? 应该更高?<br clear="none"/></span>增大到100:<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>[:</span><span>1000</span><span>],</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>100.0</span><span>,</span> <span>lmbda</span> <span>=</span> <span>20.0</span><span>,</span> \
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>[:</span><span>100</span><span>],</span> \
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/></span><br clear="none"/>结果:<br clear="none"/></pre><pre xml:space="preserve"><span>Epoch</span> <span>0</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>10</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>1</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>10</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>2</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>10</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>3</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>10</span> <span>/</span> <span>100<br clear="none"/><br clear="none"/>结果非常差, 也许结果学习率应该更低? =10<br clear="none"/><br clear="none"/><br clear="none"/></span></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>[:</span><span>1000</span><span>],</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>1.0</span><span>,</span> <span>lmbda</span> <span>=</span> <span>20.0</span><span>,</span> \
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>[:</span><span>100</span><span>],</span> \
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/></span>结果好很多:
<span>Epoch</span> <span>0</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>62</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>1</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>42</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>2</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>43</span> <span>/</span> <span>100</span>

<span>Epoch</span> <span>3</span> <span>training</span> <span>complete</span>
<span>Accuracy</span> <span>on</span> <span>evaluation</span> <span>data</span><span>:</span> <span>61</span> <span>/</span> <span>100</span>
</pre><div><span><br clear="none"/></span></div><pre xml:space="preserve"><br clear="none"/>假设保持其他参数不变: 30 epochs, mini-batch size: 10, <span>λ</span><span>=</span><span>5.0</span><br clear="none"/>实验学习率=0.025, 0.25, 2.5<br clear="none"/><br clear="none"/></pre></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [27].png" type="image/png" width="520px"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>如果学习率太大, 可能造成越走越高, 跳过局部最低点</div><div>太小, 学习可能太慢</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [28].png" type="image/png"/><br clear="none"/></div><div>对于学习率, 可以从0.001, 0.01, 0.1, 1, 10 开始尝试, 如果发现cost开始增大, 停止, 实验更小的微调</div><div><br clear="none"/></div><div>对于MNIST, 先找到0.1, 然后0.5, 然后0.25</div><div><br clear="none"/></div><div>对于提前停止学习的条件设置, 如果accuracy在一段时间内变化很小 (不是一两次)</div><div><br clear="none"/></div><div>之前一直使用学习率是常数, 可以开始设置大一下, 后面逐渐减少: 比如开始设定常数, 直到在验证集上准确率开始下降, 减少学习率 (/2, /3)</div><div><br clear="none"/></div><div><strong>对于regularization parameter <span><span><span><span>λ:</span></span></span></span></strong></div><div><br clear="none"/></div><div>先不设定regularization, 把学习率调整好, 然后再开始实验<span><span><span><span>λ, 1.0, 10, 100..., 找到合适的, 再微调</span></span></span></span></div><div><span><span><span><span><br clear="none"/></span></span></span></span></div><div><span><span><span><span><br clear="none"/></span></span></span></span></div><div><strong>对于mini-batch size:</strong></div><div><strong><br clear="none"/></strong></div><div>太小: 没有充分利用矩阵计算的library和硬件的整合的快速计算</div><div>太大: 更新权重和偏向不够频繁</div><div><br clear="none"/></div><div>好在mini-batch size和其他参数变化相对独立, 所以不用重新尝试, 一旦选定</div><div><br clear="none"/></div><div><strong>自动搜索:</strong></div><div>网格状搜索各种参数组合 (grid search)</div><div><span>2012*</span><span>*<a href="http://dl.acm.org/citation.cfm?id=2188395" shape="rect" target="_blank">Random search for hyper-parameter optimization</a>, by James Bergstra and Yoshua Bengio (2012).</span><span> by James Bergstra and Yoshua Bengio</span></div><div><span><br clear="none"/></span></div><div><span>1998 paper*<span>*<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" shape="rect" target="_blank">Efficient BackProp</a>, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller (1998)</span><span> by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller.</span></span></div><div><span><br clear="none"/></span></div><div>参数之前会互相影响</div><div><br clear="none"/></div><div><br clear="none"/></div><div>如何选择合适的hyper-parameters仍是一个正在研究的课题</div><div><br clear="none"/></div><div>随机梯度下降有没有其他变种: Hessian 优化, Momentum-based gradient descent</div><div><br clear="none"/></div><div>除了sigmoid, 其他人工神经网络的模型?</div><div><br clear="none"/></div><div>tanh</div><div><br clear="none"/></div><div><span>tanh</span><span>(</span><span>w</span><span>⋅</span><span>x</span><span>+</span><span>b</span><span>)</span></div></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-23 16-15-36.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-23 16-16-18.png" type="image/png"/></div><div>tanh 只是一个重新调节过度量后的 sigmoid函数</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-23 16-18-23.png" type="image/png"/></div><div><br clear="none"/></div><div>-1 到 1 之间, 不像 sigmoid 在 0, 1 之间, 所以输入要转化到-1, 1之间</div><div><br clear="none"/></div><div><br clear="none"/></div><div>rectified linear 神经元:</div><div><br clear="none"/></div><div><span>max</span><span>(</span><span>0</span><span>,</span><span>w</span><span>⋅</span><span>x</span><span>+</span><span>b</span><span>)</span></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-23 16-26-24.png" type="image/png"/></div><div>像sigmoid, tanh一样, 也可以扑模拟何函数</div><div>优势: 增加权重不会引起饱和, 但加权的输入如果是负数, gradient就为0</div><div><br clear="none"/></div><div>要靠实验比较rectified linear和sigmoid, tanh的好坏</div><div><br clear="none"/></div><div>目前神经网络还有很多方面理论基础需要研究, 为什么学习能力强, 现在的一些实验表明结果比较好, 但发展底层理论基础还有很长的路要走</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="398"/>

<div>
<span><div>用不同的初始化权重方法对比:</div><div><br clear="none"/></div><div>之前的方法: N(0, 1)</div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span>
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>import</span> <span>network2</span>
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>],</span> <span>cost</span><span>=</span><span>network2</span><span>.</span><span>CrossEntropyCost</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>large_weight_initializer</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.1</span><span>,</span> <span>lmbda</span> <span>=</span> <span>5.0</span><span>,</span>
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>,</span> 
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/></span><br clear="none"/><br clear="none"/>新方法: N(0, 1/sqrt(n_in))<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>],</span> <span>cost</span><span>=</span><span>network2</span><span>.</span><span>CrossEntropyCost</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.1</span><span>,</span> <span>lmbda</span> <span>=</span> <span>5.0</span><span>,</span>
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>,</span> 
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [29].png" type="image/png" width="520px"/><br clear="none"/>两种方法都得到了高于96%的accuracy<br clear="none"/><br clear="none"/>但是新方法更快把精确度提高了 (87 vs 93)<br clear="none"/><br clear="none"/>对于隐藏层有100个神经元的对比:<br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [30].png" type="image/png" width="520px"/><br clear="none"/><br clear="none"/><br clear="none"/>从这个例子中看到新的初始化方法只是增快了学习的速率, 最终表现是一样的, 但在有些神经网络中, 新的初始化权重的方法会提高最终的accuracy<br clear="none"/><br clear="none"/><br clear="none"/><strong>实现提高版本的神经网络算法来识别手写数字:<br clear="none"/><br clear="none"/></strong>复习之前原始的版本: Network.py<br clear="none"/><br clear="none"/>我们从以下方面做了提高:<br clear="none"/><br clear="none"/>Cost函数: cross-entropy<br clear="none"/>Regularization: L1, L2<br clear="none"/>Softmax layer<br clear="none"/>初始化 1/sqrt(n_in)<br clear="none"/><br clear="none"/><br clear="none"/></span></pre><pre xml:space="preserve"><span>class</span> <span>Network</span><span>(</span><span>object</span><span>):</span>

    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>sizes</span><span>,</span> <span>cost</span><span>=</span><span>CrossEntropyCost</span><span>):</span>
        <span>self</span><span>.</span><span>num_layers</span> <span>=</span> <span>len</span><span>(</span><span>sizes</span><span>)</span>
        <span>self</span><span>.</span><span>sizes</span> <span>=</span> <span>sizes</span>
        <span>self</span><span>.</span><span>default_weight_initializer</span><span>()</span>
        <span>self</span><span>.</span><span>cost</span><span>=</span><span>cost<br clear="none"/><br clear="none"/></span>对比初始化的不同方法:<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>     def</span> <span>default_weight_initializer</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>biases</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>y</span><span>,</span> <span>1</span><span>)</span> <span>for</span> <span>y</span> <span>in</span> <span>self</span><span>.</span><span>sizes</span><span>[</span><span>1</span><span>:]]</span>
        <span>self</span><span>.</span><span>weights</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>y</span><span>,</span> <span>x</span><span>)</span><span>/</span><span>np</span><span>.</span><span>sqrt</span><span>(</span><span>x</span><span>)</span> 
                        <span>for</span> <span>x</span><span>,</span> <span>y</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>sizes</span><span>[:</span><span>-</span><span>1</span><span>],</span> <span>self</span><span>.</span><span>sizes</span><span>[</span><span>1</span><span>:])]<br clear="none"/><br clear="none"/></span></pre><pre xml:space="preserve">    <span>def</span> <span>large_weight_initializer</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>biases</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>y</span><span>,</span> <span>1</span><span>)</span> <span>for</span> <span>y</span> <span>in</span> <span>self</span><span>.</span><span>sizes</span><span>[</span><span>1</span><span>:]]</span>
        <span>self</span><span>.</span><span>weights</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>y</span><span>,</span> <span>x</span><span>)</span> 
                        <span>for</span> <span>x</span><span>,</span> <span>y</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>sizes</span><span>[:</span><span>-</span><span>1</span><span>],</span> <span>self</span><span>.</span><span>sizes</span><span>[</span><span>1</span><span>:])]</span></pre><pre xml:space="preserve"><span><br clear="none"/></span></pre><pre xml:space="preserve"><span><br clear="none"/></span><br clear="none"/></pre><pre xml:space="preserve"><span>对于Cost函数:<br clear="none"/><br clear="none"/></span></pre><div><pre xml:space="preserve"><span>class</span> <span>CrossEntropyCost</span><span>(</span><span>object</span><span>):</span>

    <span>@staticmethod</span>
    <span>def</span> <span>fn</span><span>(</span><span>a</span><span>,</span> <span>y</span><span>):</span>
        <span>return</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>np</span><span>.</span><span>nan_to_num</span><span>(</span><span>-</span><span>y</span><span>*</span><span>np</span><span>.</span><span>log</span><span>(</span><span>a</span><span>)</span><span>-</span><span>(</span><span>1</span><span>-</span><span>y</span><span>)</span><span>*</span><span>np</span><span>.</span><span>log</span><span>(</span><span>1</span><span>-</span><span>a</span><span>)))</span>

    <span>@staticmethod</span>
    <span>def</span> <span>delta</span><span>(</span><span>z</span><span>,</span> <span>a</span><span>,</span> <span>y</span><span>):</span>
        <span>return</span> <span>(</span><span>a</span><span>-</span><span>y</span><span>)</span>
</pre><div><span><br clear="none"/></span></div></div><p> </p><p><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-26-08.png" type="image/png"/></p><p>为什么把cost实现在一个类里面而不是一个function?</p><p>计算cost有两个作用:</p><p>1. 衡量网络输出的值和理想预期值的匹配程度</p><p>2. 在用backprogapation计算偏导数的时候, 需要计算 </p><pre xml:space="preserve">          <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 18-50-07.png" type="image/png"/><br clear="none"/>取决于cost function, 对于cross-entropy:</pre></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 18-50-37.png" type="image/png"/></div><div>所以写在一个类里面</div><div><br clear="none"/></div><div>对于老的二次cost:</div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>class</span> <span>QuadraticCost</span><span>(</span><span>object</span><span>):</span>

    <span>@staticmethod</span>
    <span>def</span> <span>fn</span><span>(</span><span>a</span><span>,</span> <span>y</span><span>):</span>
        <span>return</span> <span>0.5</span><span>*</span><span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>a</span><span>-</span><span>y</span><span>)</span><span>**</span><span>2</span>

    <span>@staticmethod</span>
    <span>def</span> <span>delta</span><span>(</span><span>z</span><span>,</span> <span>a</span><span>,</span> <span>y</span><span>):</span>
        <span>return</span> <span>(</span><span>a</span><span>-</span><span>y</span><span>)</span> <span>*</span> <span>sigmoid_prime</span><span>(</span><span>z</span><span>)<br clear="none"/><br clear="none"/><br clear="none"/></span>从Eclipse里面看整体Network2.py代码<br clear="none"/><br clear="none"/>加上了一些flags<br clear="none"/><br clear="none"/>加上了save<br clear="none"/><br clear="none"/>import mnist_loader<br clear="none"/>training_data, validation_data, test_data = mnist_loader.load_data_wrapper()<br clear="none"/><br clear="none"/>import network2<br clear="none"/>net = network2.Network([784, 30, 10], cost = network2.CrossEntropyCost)<br clear="none"/><br clear="none"/>net.SGD(training_data, 30, 10, 0.5, 5.0, evaluation_data = validation_data, monitor_evaluation_accuracy=True, monitor_evaluation_cost=True, monitor_training_accuracy=True, monitor_training_cost=True)<br clear="none"/><br clear="none"/>Epoch 0 training complete<br clear="none"/>Cost on training data: 0.460784098781<br clear="none"/>Accuracy on training data: 47221 / 50000<br clear="none"/>Cost on evaluation data: 0.768648892938<br clear="none"/>Accuracy on evaluation data: 9449 / 10000<br clear="none"/><br clear="none"/>Epoch 1 training complete<br clear="none"/>Cost on training data: 0.428431721319<br clear="none"/>Accuracy on training data: 47640 / 50000<br clear="none"/>Cost on evaluation data: 0.830991192439<br clear="none"/>Accuracy on evaluation data: 9515 / 10000<br clear="none"/><br clear="none"/>Epoch 2 training complete<br clear="none"/>Cost on training data: 0.403506934796<br clear="none"/>Accuracy on training data: 47948 / 50000<br clear="none"/>Cost on evaluation data: 0.865928186374<br clear="none"/>Accuracy on evaluation data: 9557 / 10000<br clear="none"/><br clear="none"/>Epoch 3 training complete<br clear="none"/>Cost on training data: 0.397916271809<br clear="none"/>Accuracy on training data: 48004 / 50000<br clear="none"/>Cost on evaluation data: 0.890250486067<br clear="none"/>Accuracy on evaluation data: 9558 / 10000<br clear="none"/><br clear="none"/>Epoch 4 training complete<br clear="none"/>Cost on training data: 0.403262529832<br clear="none"/>Accuracy on training data: 48091 / 50000<br clear="none"/>Cost on evaluation data: 0.911213738748<br clear="none"/>Accuracy on evaluation data: 9561 / 10000<br clear="none"/><br clear="none"/>Epoch 5 training complete<br clear="none"/>Cost on training data: 0.421707589024<br clear="none"/>Accuracy on training data: 47937 / 50000<br clear="none"/>Cost on evaluation data: 0.935183110562<br clear="none"/>Accuracy on evaluation data: 9570 / 10000<br clear="none"/><br clear="none"/>Epoch 6 training complete<br clear="none"/>Cost on training data: 0.412316727125<br clear="none"/>Accuracy on training data: 48077 / 50000<br clear="none"/>Cost on evaluation data: 0.940574252771<br clear="none"/>Accuracy on evaluation data: 9565 / 10000<br clear="none"/><br clear="none"/>Epoch 7 training complete<br clear="none"/>Cost on training data: 0.380372945312<br clear="none"/>Accuracy on training data: 48290 / 50000<br clear="none"/>Cost on evaluation data: 0.923778317941<br clear="none"/>Accuracy on evaluation data: 9598 / 10000<br clear="none"/><br clear="none"/>Epoch 8 training complete<br clear="none"/>Cost on training data: 0.416246864998<br clear="none"/>Accuracy on training data: 48035 / 50000<br clear="none"/>Cost on evaluation data: 0.962819321069<br clear="none"/>Accuracy on evaluation data: 9566 / 10000<br clear="none"/><br clear="none"/>Epoch 9 training complete<br clear="none"/>Cost on training data: 0.423953651473<br clear="none"/>Accuracy on training data: 48018 / 50000<br clear="none"/>Cost on evaluation data: 0.966696944506<br clear="none"/>Accuracy on evaluation data: 9556 / 10000<br clear="none"/><br clear="none"/>Epoch 10 training complete<br clear="none"/>Cost on training data: 0.372232139986<br clear="none"/>Accuracy on training data: 48363 / 50000<br clear="none"/>Cost on evaluation data: 0.92144625029<br clear="none"/>Accuracy on evaluation data: 9603 / 10000<br clear="none"/><br clear="none"/>Epoch 11 training complete<br clear="none"/>Cost on training data: 0.389443902734<br clear="none"/>Accuracy on training data: 48264 / 50000<br clear="none"/>Cost on evaluation data: 0.943950269739<br clear="none"/>Accuracy on evaluation data: 9605 / 10000<br clear="none"/><br clear="none"/>Epoch 12 training complete<br clear="none"/>Cost on training data: 0.376081041422<br clear="none"/>Accuracy on training data: 48414 / 50000<br clear="none"/>Cost on evaluation data: 0.938832360561<br clear="none"/>Accuracy on evaluation data: 9614 / 10000<br clear="none"/><br clear="none"/>Epoch 13 training complete<br clear="none"/>Cost on training data: 0.374891623652<br clear="none"/>Accuracy on training data: 48329 / 50000<br clear="none"/>Cost on evaluation data: 0.936589349746<br clear="none"/>Accuracy on evaluation data: 9620 / 10000<br clear="none"/><br clear="none"/>Epoch 14 training complete<br clear="none"/>Cost on training data: 0.391965148858<br clear="none"/>Accuracy on training data: 48251 / 50000<br clear="none"/>Cost on evaluation data: 0.955904265264<br clear="none"/>Accuracy on evaluation data: 9589 / 10000<br clear="none"/><br clear="none"/>Epoch 15 training complete<br clear="none"/>Cost on training data: 0.387328845894<br clear="none"/>Accuracy on training data: 48342 / 50000<br clear="none"/>Cost on evaluation data: 0.943383118258<br clear="none"/>Accuracy on evaluation data: 9640 / 10000<br clear="none"/><br clear="none"/>Epoch 16 training complete<br clear="none"/>Cost on training data: 0.403566245251<br clear="none"/>Accuracy on training data: 48168 / 50000<br clear="none"/>Cost on evaluation data: 0.983612747694<br clear="none"/>Accuracy on evaluation data: 9563 / 10000<br clear="none"/><br clear="none"/>Epoch 17 training complete<br clear="none"/>Cost on training data: 0.391745519598<br clear="none"/>Accuracy on training data: 48303 / 50000<br clear="none"/>Cost on evaluation data: 0.960435104507<br clear="none"/>Accuracy on evaluation data: 9611 / 10000<br clear="none"/><br clear="none"/>Epoch 18 training complete<br clear="none"/>Cost on training data: 0.400147700417<br clear="none"/>Accuracy on training data: 48253 / 50000<br clear="none"/>Cost on evaluation data: 0.97861566065<br clear="none"/>Accuracy on evaluation data: 9591 / 10000<br clear="none"/><br clear="none"/>Epoch 19 training complete<br clear="none"/>Cost on training data: 0.392736589877<br clear="none"/>Accuracy on training data: 48287 / 50000<br clear="none"/>Cost on evaluation data: 0.9744201917<br clear="none"/>Accuracy on evaluation data: 9562 / 10000<br clear="none"/><br clear="none"/>Epoch 20 training complete<br clear="none"/>Cost on training data: 0.367081932532<br clear="none"/>Accuracy on training data: 48437 / 50000<br clear="none"/>Cost on evaluation data: 0.937185065265<br clear="none"/>Accuracy on evaluation data: 9640 / 10000<br clear="none"/><br clear="none"/>Epoch 21 training complete<br clear="none"/>Cost on training data: 0.381100563917<br clear="none"/>Accuracy on training data: 48372 / 50000<br clear="none"/>Cost on evaluation data: 0.953269176913<br clear="none"/>Accuracy on evaluation data: 9615 / 10000<br clear="none"/><br clear="none"/>Epoch 22 training complete<br clear="none"/>Cost on training data: 0.381446338812<br clear="none"/>Accuracy on training data: 48290 / 50000<br clear="none"/>Cost on evaluation data: 0.957448205047<br clear="none"/>Accuracy on evaluation data: 9587 / 10000<br clear="none"/><br clear="none"/>Epoch 23 training complete<br clear="none"/>Cost on training data: 0.371327393124<br clear="none"/>Accuracy on training data: 48434 / 50000<br clear="none"/>Cost on evaluation data: 0.949700025345<br clear="none"/>Accuracy on evaluation data: 9610 / 10000<br clear="none"/><br clear="none"/>Epoch 24 training complete<br clear="none"/>Cost on training data: 0.416443559054<br clear="none"/>Accuracy on training data: 48222 / 50000<br clear="none"/>Cost on evaluation data: 0.989227239735<br clear="none"/>Accuracy on evaluation data: 9591 / 10000<br clear="none"/><br clear="none"/>Epoch 25 training complete<br clear="none"/>Cost on training data: 0.376334274965<br clear="none"/>Accuracy on training data: 48397 / 50000<br clear="none"/>Cost on evaluation data: 0.953265649043<br clear="none"/>Accuracy on evaluation data: 9610 / 10000<br clear="none"/><br clear="none"/>Epoch 26 training complete<br clear="none"/>Cost on training data: 0.373212737488<br clear="none"/>Accuracy on training data: 48476 / 50000<br clear="none"/>Cost on evaluation data: 0.946853755663<br clear="none"/>Accuracy on evaluation data: 9609 / 10000<br clear="none"/><br clear="none"/>Epoch 27 training complete<br clear="none"/>Cost on training data: 0.36773451426<br clear="none"/>Accuracy on training data: 48499 / 50000<br clear="none"/>Cost on evaluation data: 0.942795806645<br clear="none"/>Accuracy on evaluation data: 9622 / 10000<br clear="none"/><br clear="none"/>Epoch 28 training complete<br clear="none"/>Cost on training data: 0.369747119451<br clear="none"/>Accuracy on training data: 48494 / 50000<br clear="none"/>Cost on evaluation data: 0.937987364862<br clear="none"/>Accuracy on evaluation data: 9632 / 10000<br clear="none"/><br clear="none"/>Epoch 29 training complete<br clear="none"/>Cost on training data: 0.386978483432<br clear="none"/>Accuracy on training data: 48328 / 50000<br clear="none"/>Cost on evaluation data: 0.956798203553<br clear="none"/>Accuracy on evaluation data: 9604 / 10000<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/></pre></div></span>
</div>
<hr>
<a name="404"/>

<div>
<span><div><br clear="none"/></div><div><strong>正态分布</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [1].jpg" type="image/jpeg"/></div><div>如果均值=0, 方差=1, 标准正太分布</div><div><br clear="none"/></div><div>智商, 均值= 100, 标准差= 15</div><div><br clear="none"/></div><div><br clear="none"/></div><div>均值: mean, average   x_bar = sum(xi, i = 1 ....n) / n,  一组数据它的中心趋势的衡量</div><div>标准差: standard deviation,  sigma = sqrt(sum((xi - x_bar)^2, (i=1, .... n)) / (n-1))</div><div>方差: 标准差^2 = 方差</div><div><br clear="none"/></div><div>X = (x1, x2, x3, x3, x5) = 1, 2, 3, 4, 5</div><div><br clear="none"/></div><div>x_bar = (1+2+3+4+5)/5= 15/5 =3</div><div><br clear="none"/></div><div>sqrt(((1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2) / (5 - 1))</div><div>= sqrt( 4 + 1 + 0 + 1+ 4) / 4))</div><div>= sqrt(2.5)</div><div>= 1.58</div><div><br clear="none"/></div><div> 方差 = 1.58^2 = 2.5</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>假设如下一个神经网络:</div><div><img src="https://www.evernote.com/shard/s27/res/c327173e-8dbf-4750-8665-9064ff913110/screenshot-neuralnetworksanddeeplearning.com%202015-09-22%2015-50-02.png"></img></div><div><br clear="none"/></div><div><br clear="none"/></div><div>输入x: 一半是0, 一半是1, 一共1000个输入层</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-56-43.png" type="image/png"/></div><div><br clear="none"/></div><div>结果: 一半消失了, 剩下的500个1, 加上b, 分布: 标准差: sqrt(501) = 22.4</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-57-55.png" type="image/png"/></div><div>z很多都远远大于1, 或者远远小于-1, 根据sigmoid函数</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [31].png" type="image/png"/></div><div> 输出的值都接近0和1, 当权重变化时, 更新量很小, 对于更新后面的层, 更新量很小,学习很慢,</div><div><br clear="none"/></div><div>使隐藏层饱和了, 跟之前我们说的输出层饱和问题相似, 对于输出层,我们用改进的cost函数,比如cross-entropy, 但是对于隐藏层, 我们无法通过cost函数来改进</div><div><br clear="none"/></div><div>更好的方法来初始化权重?</div><div><br clear="none"/></div><div>从正态分布均值=0, 标准差差等于 1/sqrt(n_in)</div><div><br clear="none"/></div><div>重复500个1, 500个0作为输入时, z分布的标准差变成了sqrt(3/2) = 1.22</div><div><br clear="none"/></div><div>证明: 标准差 = 1 / sqrt(n_in) =&gt; 方差 = 1 / n_in</div><div>n_in = 1000, 方差 = 1/ 1000</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-56-43 [1].png" type="image/png"/>,</div><div>不等于0的x有500, z的方差500*(1/1000) + b = 1/2 + 1 = 3/2</div><div>z的标准差变成了 sqrt(3/2)</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 16-29-47.png" type="image/png"/></div><div><br clear="none"/></div><div>大部分z在1和-1之间, 神经元没有饱和, 学习过程不会被减慢</div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="410"/>

<div>
<span><div><strong>L1 regularization: </strong></div><div><br clear="none"/><strong><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 14-37-38.png" type="image/png"/></strong></div><div>跟L2 regularization 相似, 但不一样</div><div><br clear="none"/></div><div>求偏导:</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 14-39-28.png" type="image/png"/></div><div>sgn(w) 指的是w的符号, +1如果w是正数, -1如果w是负数</div><div><br clear="none"/></div><div>权重更新法则:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 14-41-33.png" type="image/png"/><br clear="none"/></div><div>对比之前的L2更新法则:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 14-42-20.png" type="image/png"/><br clear="none"/></div><div><br clear="none"/></div><div>都是减小权重, 方法不同: </div><div>L1减少一个常量, L2减少权重的一个固定比例</div><div>如果权重本身很大, L1减少的比L2少很多</div><div>如果权重本身很小, L1减少的更多</div><div><br clear="none"/></div><div>L1倾向于集中在少部分重要的连接上</div><div><br clear="none"/></div><div>当w=0, 偏导数<span>∂</span><span>C</span><span><span><span>/</span></span></span><span>∂</span><span>w无意义, 因为|w|的形状在w=0时是一个V字形尖锐的拐点.  </span></div><div><span>所以, 当w=0时,我们就使用un-regulazied表达式, sgn(0) = 0. 本来regularization的目的就是减小权重, 当权重=0时,无需减少</span></div><div><span><br clear="none"/></span></div><div><strong>Dropout:</strong></div><div><strong><br clear="none"/></strong></div><div>和L1, L2 regularization非常不同, 不是针对cost函数增加一项,而是对神经网络本身的结构做改变</div><div><br clear="none"/></div><div>假设我们有一个神经网络</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-07-14.png" type="image/png"/><br clear="none"/></div><div>通常, 我们根据输入的x,正向更新神经网络,算出输出值,然后反向根据backpropagation来更新权重和偏向</div><div><br clear="none"/></div><div>但是, dropout不同:</div><div><br clear="none"/></div><div>开始, 删除掉隐藏层随机选取的一半神经元</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-09-23.png" type="image/png"/><br clear="none"/></div><div>然后, 在这个更改过的神经网络上正向和反向更新, 利用一个mini-batch</div><div><br clear="none"/></div><div>然后, 恢复之前删除过的神经元, 重新随机选择一半神经元删除, 正向, 反向, 更新w,b</div><div><br clear="none"/></div><div>重复此过程</div><div><br clear="none"/></div><div>最后,学习出来的神经网络中的每个神经元都是在只有一半神经元的基础上学习的, 当所有神经元被恢复后, 为了补偿, 我们把隐藏层的所有权重减半</div><div><br clear="none"/></div><div><br clear="none"/></div><div>为什么dropout可以减少overfitting?</div><div><br clear="none"/></div><div>假设我们对于同一组训练数据, 利用不同的神经网络来训练, 训练完成之后, 求输出的平均值, 这样可以减少overfitting</div><div><br clear="none"/></div><div>Dropout和这个是同样的道理, 每次扔到一半隐藏层的神经元, 相当于我们在不同的神经网络上训练了</div><div><br clear="none"/></div><div>减少了神经元的依赖性, 也就是每个神经元不能依赖于某个或者某几个其他神经元, 迫使神经网聚学习更加和其他神经元联合起来的更加健硕的特征</div><div><br clear="none"/></div><div>介绍dropout的文章, 对于以前MNIST最高的accuracy是98.4%, 利用dropout, 提高到98.7%</div><div><br clear="none"/></div><div><strong>人工扩大训练集:</strong></div><div><strong><br clear="none"/></strong></div><div>之前看到了只有1000张图和50,000张图片训练的区别</div><div><br clear="none"/></div><div>比较一下随着训练集的增大, accuracy的变化</div><div><br clear="none"/></div><div>参数: 隐藏层: 30个神经元, mini-batch size: 10, 学习率0.5, <span><br clear="none"/>λ</span><span>=</span><span>5.0 (对于整个训练集, 对应减少当训练集减少时), 当, cross-entropy cost</span></div><div><span><br clear="none"/></span></div><div><span>训练30个epochs</span></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-33-13.png" type="image/png"/></div><div>log scale:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [32].png" type="image/png" width="520px"/></div><div><br clear="none"/></div><div>accuracy一直在增加, 可以想象如果大规模增加训练集, 可以达到更高准确率</div><div><br clear="none"/></div><div>有label的训练集通常不好获得, 一种方法, 人工产生更多训练数据:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [33].png" type="image/png" width="120px"/>转15度<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [34].png" type="image/png" width="120px"/></div><div><br clear="none"/></div><div>旋转不同角度, 增加很多训练数据</div><div><br clear="none"/></div><div>比如: MNIST, 一个隐藏层有800个神经元的网络, 98.4%, 人工增加数据后, 达到98.9%, 发明了一下人工会改变图像的模拟方法进一步增大训练集, 准确率达到了 99.3%</div><div><br clear="none"/></div><div>增大时, 要模拟现实世界中这种数据可能出现的变化, 来概括更广</div><div><br clear="none"/></div><div><br clear="none"/></div><div>对比神经网络和SVM:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [35].png" type="image/png" width="520px"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>比较两个算法的时候, 是否使用同样的训练集很重要, 因为训练集的增大可以提高accuracy</div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>初始化权重:</strong></div><div><br clear="none"/></div><div>之前: 随机从正态分布中产生(均值0, 方差1)</div><div><br clear="none"/></div><div><br clear="none"/></div><div>正态分布:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image.gif" type="image/gif"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>假设如下一个神经网络:</div><div><img src="https://www.evernote.com/shard/s27/res/c327173e-8dbf-4750-8665-9064ff913110/screenshot-neuralnetworksanddeeplearning.com%202015-09-22%2015-50-02.png"></img></div><div><br clear="none"/></div><div><br clear="none"/></div><div>输入x: 一半是0, 一半是1, 一共1000个输入层</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-56-43 [2].png" type="image/png"/></div><div><br clear="none"/></div><div>结果: 一半消失了, 剩下的500个1, 加上b, 分布: 标准差: sqrt(501) = 22.4</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-57-55 [1].png" type="image/png"/></div><div>z很多都远远大于1, 或者远远小于-1, 根据sigmoid函数</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [36].png" type="image/png"/></div><div> 输出的值都接近0和1, 当权重变化事, 更新量很小, 对于更新后面的层, 更新量很小,学习很慢,</div><div><br clear="none"/></div><div>使隐藏层饱和了, 跟之前我们说的输出层饱和问题相似, 对于输出层,我们用改进的cost函数,比如cross-entropy, 但是对于隐藏层, 我们无法通过cost函数来改进</div><div><br clear="none"/></div><div>更好的方法来初始化权重?</div><div><br clear="none"/></div><div>从正态分布均值=0, 标准差差等于 1/sqrt(n_in)</div><div><br clear="none"/></div><div>重复500个1, 500个0作为输入时, z分布的标准差变成了sqrt(3/2) = 1.22</div><div><br clear="none"/></div><div>证明: 标准差 = 1 / sqrt(n_in) =&gt; 方差 = 1 / n_in</div><div>n_in = 1000, 方差 = 1/ 1000</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 15-56-43 [3].png" type="image/png"/>,</div><div>不等于0的x有500, z的方差500*(1/1000) + b = 1/2 + 1 = 3/2</div><div>z的标准差变成了 sqrt(3/2)</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-22 16-29-47 [1].png" type="image/png"/></div><div><br clear="none"/></div><div>大部分z在1和-1之间, 神经元没有饱和, 学习过程不会被减慢</div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="432"/>

<div>
<span><div>上节课: 增加训练数据集的量是减少overfitting的途径之一</div><div><br clear="none"/></div><div>减小神经网络的规模, 但是更深层更大的网络潜在有更强的学习能力</div><div><br clear="none"/></div><div>即使对于固定的神经网络和固定的训练集, 仍然可以减少overfitting:</div><div><br clear="none"/></div><div><strong>Regularization</strong></div><div><br clear="none"/></div><div>最常见的一种regularization: (weight decay)L2 regularization</div><div><br clear="none"/></div><div>Regularized cross-entropy:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-08-36.png" type="image/png"/><br clear="none"/></div><div>增加了一项: 权重之和 (对于神经网络里面的所有权重w相加)</div><div><span> </span><span><span><span><span><span><span>λ</span><span>&gt;</span><span>0: regularization 参数</span></span></span></span></span></span></div><div><span><span><span><span><span><span>n: 训练集包含实例个数</span></span></span></span></span></span></div><div><span><span><span><span><span><span><br clear="none"/></span></span></span></span></span></span></div><div>对于二次cost,</div><div>Regularized quadratic cost:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-13-43.png" type="image/png"/><br clear="none"/></div><div>对于以上两种情况, 可以概括表示为:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-14-46.png" type="image/png"/><br clear="none"/></div><div>Regularization的Cost偏向于让神经网络学习比较小的权重w, 除非第一项的Co明显减少. </div><div><br clear="none"/></div><div><span>λ: 调整两项的相对重要程度, 较小的λ倾向于让第一项Co最小化. 较大的λ倾向与最小化增大的项(权重之和).</span></div><div><span><br clear="none"/></span></div><div><span><br clear="none"/></span></div><div><span>对以上公式求偏导数:</span></div><div><span><br clear="none"/></span></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-21-05.png" type="image/png"/><br clear="none"/></div><div>以上两个偏导数可以用之前介绍的backpropagation算法求得:</div><div>添加了一个项: <span><span><span><br clear="none"/></span></span></span></div><div>                       <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-22-55.png" type="image/png"/><br clear="none"/></div><div>对于偏向b, 偏导数不变</div><div><br clear="none"/></div><div>根据梯度下降算法, 更新法则变为:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-31-38.png" type="image/png"/>        </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-32-09.png" type="image/png"/><br clear="none"/></div><div>对于随机梯度下降(stochastic gradient descent):</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-36-13.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-36-49.png" type="image/png"/></div><div>求和是对于一个mini-batch里面所有的x</div><div><br clear="none"/></div><div>实验:</div><div><br clear="none"/></div><div>隐藏层: 30个神经元, mini-batch size: 10, 学习率: 0.5, cross-entropy</div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span> 
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()</span> 
<span>&gt;&gt;&gt;</span> <span>import</span> <span>network2</span> 
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>],</span> <span>cost</span><span>=</span><span>network2</span><span>.</span><span>CrossEntropyCost</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>large_weight_initializer</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>[:</span><span>1000</span><span>],</span> <span>400</span><span>,</span> <span>10</span><span>,</span> <span>0.5</span><span>,</span>
<span>...</span> <span>evaluation_data</span><span>=</span><span>test_data</span><span>,</span> <span>lmbda</span> <span>=</span> <span>0.1</span><span>,</span>
<span>...</span> <span>monitor_evaluation_cost</span><span>=</span><span>True</span><span>,</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>,</span>
<span>...</span> <span>monitor_training_cost</span><span>=</span><span>True</span><span>,</span> <span>monitor_training_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [37].png" type="image/png" width="520px"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/>    但是这次accuracy在test data上面持续增加:<br clear="none"/><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [38].png" type="image/png" width="520px"/><br clear="none"/><br clear="none"/>    最高的accuracy也增加了, 说明regularization减少了overfitting<br clear="none"/><br clear="none"/><br clear="none"/>   如果用50,000张训练集:<br clear="none"/>     同样的参数: 30 epochs, 学习率 0.5, mini-batch size: 10<br clear="none"/>     需要改变<span>λ, 因为n从1,000变到50,000了 </span><br clear="none"/><br clear="none"/></span></pre></div><div>              <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 17-52-16.png" type="image/png"/></div><div>            变了, 所以需要增大λ, 增大到 5.0<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>large_weight_initializer</span><span>()</span></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.5</span><span>,</span>
<span>...</span> <span>evaluation_data</span><span>=</span><span>test_data</span><span>,</span> <span>lmbda</span> <span>=</span> <span>5.0</span><span>,</span>
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>,</span> <span>monitor_training_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/><br clear="none"/>结果好很多, accuracy对于测试集提高了, 两条曲线之间的距离大大减小<br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [39].png" type="image/png" width="520px"/><br clear="none"/><br clear="none"/>如果用隐藏层100个神经元<br clear="none"/></span></pre><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>100</span><span>,</span> <span>10</span><span>],</span> <span>cost</span><span>=</span><span>network2</span><span>.</span><span>CrossEntropyCost</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>large_weight_initializer</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.5</span><span>,</span> <span>lmbda</span><span>=</span><span>5.0</span><span>,</span>
<span>...</span> <span>evaluation_data</span><span>=</span><span>validation_data</span><span>,</span>
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)</span>
</pre><div><span><br clear="none"/></span></div></div><p> </p><p>最终结果在测试集上accuracy达到97.92, 比隐藏层30个神经元提高很多</p><p>如果调整优化一下参数 用 学习率=0.1, λ=5.0, 只需要30个epoch, 准确率就超过了98%,达到了98.04%</p><p><br clear="none"/></p><p>加入regularization不仅减小了overfitting, 还对避免陷入局部最小点 (local minimum), 更容易重现实验结果</p><p><br clear="none"/></p><p><strong>为什么Regularization可以减少overfitting?</strong></p><p><strong><br clear="none"/></strong></p><p>假设一个简单数据集</p><p><br clear="none"/></p><p><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 18-02-07.png" type="image/png"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 18-04-10.png" type="image/png"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 18-05-09.png" type="image/png"/><br clear="none"/></p></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 18-06-06.png" type="image/png"/></div><div> y = 2x</div><div><br clear="none"/></div><div>那个模型更好?</div><div><br clear="none"/></div><div>y=2x更简单,仍然很好描述了数据, 巧合的概率很小,所以我们偏向y=2x</div><div>x^9的模型更可能是对局部带有数据噪音的扑捉</div><div><br clear="none"/></div><div>在神经网络中:</div><div>Regularized网络更鼓励小的权重, 小的权重的情况下, x一些随机的变化不会对神经网络的模型造成太大影响, 所以更小可能受到数据局部噪音的影响.</div><div><br clear="none"/></div><div>Un-regularized神经网路, 权重更大, 容易通过神经网络模型比较大的改变来适应数据,更容易学习到局部数据的噪音</div><div><br clear="none"/></div><div>Regularized更倾向于学到更简单一些的模型</div><div><br clear="none"/></div><div>简单的模型不一定总是更好,要从大量数据实验中获得,目前添加regularization可以更好的泛化更多的从实验中得来,理论的支持还在研究之中</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="450"/>

<div>
<span><div>用cross-entropy来识别MNIST数字</div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span>
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>import</span> <span>network2</span>
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>],</span> <span>cost</span><span>=</span><span>network2</span><span>.</span><span>CrossEntropyCost</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>large_weight_initializer</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.5</span><span>,</span> <span>evaluation_data</span><span>=</span><span>test_data</span><span>,</span>
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>)<br clear="none"/><br clear="none"/><br clear="none"/><span>net.large_weight_initializer() 跟之前初始化是一样的,专门标记一下是因为之后我们会用其他方法来初始化<br clear="none"/></span></span><br clear="none"/>演示程序和之前的二次cost函数对比: 隐藏层30, 隐藏层100<br clear="none"/><br clear="none"/>结果提高了<br clear="none"/><br clear="none"/>cross-entropy: 信息论里面的概念, 意外的程度, 对于神经网络算出来的值和真实值之间<br clear="none"/><br clear="none"/><br clear="none"/><strong>Softmax<br clear="none"/></strong><br clear="none"/>另外一种类型的输出层方程:<br clear="none"/><br clear="none"/>第一步 (和之前sigmoid一样): <br clear="none"/></pre></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 14-32-44.png" type="image/png"/></div><div>第二步: (和之前sigmoid不同): softmax函数</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 14-34-30.png" type="image/png"/><br clear="none"/></div><div>分母是把所有神经元的输入值加起来</div><div><br clear="none"/></div><div>演示: </div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 14-39-46.png" type="image/png"/><br clear="none"/></div><div><br clear="none"/></div><div>当最后一行z增大时,a也随之增大,其他a随之减小</div><div><br clear="none"/></div><div>事实上, 其他a减小的值总是刚好等于a4增加的值, 总和为1不变</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 14-44-01.png" type="image/png"/><br clear="none"/></div><div>Softmax的输出每个值都是大于等于0, 而且总和等于1</div><div>所以, 可以认为是概率分布</div><div><br clear="none"/></div><div>容易描述, 可以认为输出的是分类等于每个可能分类标签的概率(如 P(a(x)) = 0.8 for MNIST)</div><div><br clear="none"/></div><div>如果输出层是sigmod层, 不能默认输出总和为1, 所以不能轻易描述为概率分布</div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>是否存在学习慢的问题?</strong></div><div><strong><br clear="none"/></strong></div><div><strong>定义log-likelyhood函数: </strong><br clear="none"/></div><div><strong> </strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 15-15-36.png" type="image/png"/></div><div>假设输入是手写数字7的图片, 输出比较确定接近7, 对于对应的输出7的神经元, 概率a接近1,对数C接近0.  反之, 概率较小, 对数C就比较大. 所以适合做Cost函数</div><div><br clear="none"/></div><div>是否存在学习慢的问题取决于: </div><div><br clear="none"/></div><div>     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 15-23-07.png" type="image/png"/>      <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 15-23-35.png" type="image/png"/></div><div>求偏导数,得到:</div><div>         <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 15-24-35.png" type="image/png"/><br clear="none"/></div><div>对比之前用cross-entropy得到的偏导公式:</div><div>         <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-21 15-26-00.png" type="image/png"/><br clear="none"/></div><div>一样, 除了之前是平均值</div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>Overfitting</strong></div><div><strong><br clear="none"/></strong></div><div><strong>含义: 在训练集上表现好, 但不能泛化到更测试集上, 测试集表现不好</strong></div><div><strong><br clear="none"/></strong></div><div><strong><br clear="none"/></strong></div><div><br clear="none"/></div><div><strong><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [40].png" type="image/png" height="304" width="304"/></strong></div><div><br clear="none"/></div><div><br clear="none"/></div><div>神经网络有很多参数: 隐藏层有30个神经元的神经网络有约24,000个参数</div><div>隐藏层有100个神经元的神经网络有约80,000个参数, 一些最新的神经网络有几百万甚至上亿个参数</div><div><br clear="none"/></div><div>实验, 假设MNIST只用1,000个来训练</div><div><br clear="none"/></div><div><pre xml:space="preserve"> 
<span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span> 
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>import</span> <span>network2</span> 
<span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network2</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>],</span> <span>cost</span><span>=</span><span>network2</span><span>.</span><span>CrossEntropyCost</span><span>)</span> 
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>large_weight_initializer</span><span>()</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>[:</span><span>1000</span><span>],</span> <span>400</span><span>,</span> <span>10</span><span>,</span> <span>0.5</span><span>,</span> <span>evaluation_data</span><span>=</span><span>test_data</span><span>,</span>
<span>...</span> <span>monitor_evaluation_accuracy</span><span>=</span><span>True</span><span>,</span> <span>monitor_training_cost</span><span>=</span><span>True</span><span>)</span></pre></div><div><br clear="none"/></div><div><strong><br clear="none"/></strong></div><div><strong><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [41].png" type="image/png" width="520px"/></strong></div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong><br clear="none"/></strong></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [42].png" type="image/png" width="520px"/></div><div>              y轴标签放大了很多, 间隔实际很小, 在280个epoch之后,几乎不提高了</div><div><br clear="none"/></div><div>               出现了overfitting</div><div><br clear="none"/></div><div>              刚才是Cost on training data</div><div>              以下是Cost on test data</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [43].png" type="image/png" width="520px"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [44].png" type="image/png" width="520px"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>如何检测Overfitting何时发生?</div><div><br clear="none"/></div><div>用训练集或者测试集观测, 获取训练集或者测试集比较困难</div><div><br clear="none"/></div><div>之前我们载入数据时</div><div><br clear="none"/></div><div><pre xml:space="preserve"> 
<span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span> 
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()<br clear="none"/><br clear="none"/></span>没有用到validation_data, 这里可以用来检测overfitting<br clear="none"/><br clear="none"/>对于每一个epoch, 在validation_data上面计算分类准确率<br clear="none"/><br clear="none"/>一旦accuracy在validation_data上面充分了, 就停止训练<br clear="none"/><br clear="none"/><br clear="none"/>实验: 用50,000张图像训练, 30个神经元在隐藏层, 学习率=0.5, mini-batch: 10<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [45].png" type="image/png" width="520px"/><br clear="none"/><br clear="none"/>对于50,000张图片作为训练集来讲, 训练集和测试集上的accuracy远高于1,000张图片训练出来的<br clear="none"/><br clear="none"/>所以,增大训练集可以帮助减少overfitting<br clear="none"/><br clear="none"/><br clear="none"/></pre></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="475"/>

<div>
<span><div>我们理想情况是让神经网络学习更快</div><div><br clear="none"/></div><div>假设简单模型: 只有一个输入,一个神经元,一个输出</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-07-03.png" type="image/png"/></div><div>简单模型: 输入为1时, 输出为0</div><div><br clear="none"/></div><div><img src="https://www.evernote.com/shard/s27/res/0b68b607-89fb-4a2c-b7f2-e4e016af5054.png"></img></div><div><img src="https://www.evernote.com/shard/s27/res/8d34c87d-ebc8-4a11-8237-fdf0c31f9fb9.png"></img></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [46].png" type="image/png" style="cursor: default;"/></div><div><br clear="none"/></div><div>初始 w = 0.6, b = 0.9   初始预测的输出 a = 0.82, 需要学习</div><div>学习率: 0.15</div><div><br clear="none"/></div><div>演示: </div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-09-27.png" type="image/png"/></div><div><br clear="none"/></div><div>初始: w = 2.0, b = 2.0, 初始预测输出: 0.98, 和理想输出0差点很远<br clear="none"/>演示:</div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-14-13.png" type="image/png"/></div><div><br clear="none"/></div><div>神经网络的学习行为和人脑差的很多, 开始学习很慢, 后来逐渐增快.</div><div><br clear="none"/></div><div>为什么?</div><div><br clear="none"/></div><div>学习慢 =&gt; 偏导数 <span>∂</span><span>C</span><span><span><span>/</span></span></span><span>∂</span><span>w 和 <span>∂</span><span>C</span><span><span><span>/</span></span></span><span>∂</span><span>b 值小</span></span></div><div><span><span><br clear="none"/></span></span></div><div><span><span>计算偏导数:</span></span></div><div><span><span><br clear="none"/></span></span></div><div>回顾之前学习的Cost函数:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-17-33.png" type="image/png"/></div><div>对于一个x, y 和单个神经元:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-18-48.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-19-25.png" type="image/png"/></div><div>分别对w和b求偏导数:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-20-10.png" type="image/png"/>  x = 1, y = 0</div><div><br clear="none"/></div><div><br clear="none"/></div><div>回顾sigmoid函数</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-21-58.png" type="image/png"/></div><div> 当神经元输出接近1时, 曲线很平缓, </div><div>=&gt;</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-22-58.png" type="image/png"/> 很小, 所以学习很慢</div><div><br clear="none"/></div><div>如何增快学习?</div><div><br clear="none"/></div><div><strong>介绍cross-entropy cost 函数</strong></div><div><br clear="none"/></div><div>假设一个稍微复杂一些的神经网络</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-24-18.png" type="image/png"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-25-05.png" type="image/png"/>    <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-25-25.png" type="image/png"/></div><div>定义cross-entropy函数:</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-26-08 [1].png" type="image/png"/></div><div><br clear="none"/></div><div>为什么可以用来做cost函数?</div><div>1.  函数值大于等于0 (验证)</div><div>2.  当a=y时, cost = 0</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-28-31.png" type="image/png"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-29-08.png" type="image/png"/></div><div><br clear="none"/></div><div>用sigmoid函数定义</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-31-04.png" type="image/png"/></div><div>推出:</div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-31-32.png" type="image/png"/> 代入 上面的偏导, 得到:</div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-32-36.png" type="image/png"/></div><div>学习的快慢取决于 </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-33-27.png" type="image/png"/><br clear="none"/></div><div>也就是输出的error</div><div><br clear="none"/></div><div>好处: 错误大时,更新多,学得快. 错误小时,学习慢</div><div><br clear="none"/></div><div><br clear="none"/></div><div>对于偏向也类似:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-34-52.png" type="image/png"/></div><div><br clear="none"/></div><div>用cross-entropy 演示:</div><div><br clear="none"/></div><div>w = 0.6, b = 0.9</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-36-10.png" type="image/png"/></div><div><br clear="none"/></div><div>w = 2.0, b = 2.0</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-36-52.png" type="image/png"/></div><div>与之前的二次cost比较</div><div>学习率=0.005, 但是不是重点, 主要是速度的变化率, 也就是曲线的形状不同. </div><div><br clear="none"/></div><div>以上是对于一个单个神经元的cost, 对于多层:</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 21-40-21.png" type="image/png"/></div><div>以上把输出层所有的神经元的值加起来</div><div><br clear="none"/></div><div>总结:</div><div><br clear="none"/></div><div>cross-entropy cost几乎总是比二次cost函数好</div><div><br clear="none"/></div><div>如果神经元的方程是线性的, 用二次cost函数 (不会有学习慢的问题)</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="500"/>

<div>
<span><pre xml:space="preserve"><span><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-15 01-56-06.png" type="image/png"/><br clear="none"/>class</span> <span>Network</span><span>(</span><span>object</span><span>):</span>
<span>...</span>
    <span>def</span> <span>update_mini_batch</span><span>(</span><span>self</span><span>,</span> <span>mini_batch</span><span>,</span> <span>eta</span><span>):</span>
        <span>&quot;&quot;&quot;Update the network's weights and biases by applying</span>
<span>        gradient descent using backpropagation to a single mini batch.</span>
<span>        The &quot;mini_batch&quot; is a list of tuples &quot;(x, y)&quot;, and &quot;eta&quot;</span>
<span>        is the learning rate.&quot;&quot;&quot;</span>
        <span>nabla_b</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>shape</span><span>)</span> <span>for</span> <span>b</span> <span>in</span> <span>self</span><span>.</span><span>biases</span><span>]</span>
        <span>nabla_w</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>zeros</span><span>(</span><span>w</span><span>.</span><span>shape</span><span>)</span> <span>for</span> <span>w</span> <span>in</span> <span>self</span><span>.</span><span>weights</span><span>]</span>
        <span>for</span> <span>x</span><span>,</span> <span>y</span> <span>in</span> <span>mini_batch</span><span>:</span>
            <span>delta_nabla_b</span><span>,</span> <span>delta_nabla_w</span> <span>=</span> <span>self</span><span>.</span><span>backprop</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
            <span>nabla_b</span> <span>=</span> <span>[</span><span>nb</span><span>+</span><span>dnb</span> <span>for</span> <span>nb</span><span>,</span> <span>dnb</span> <span>in</span> <span>zip</span><span>(</span><span>nabla_b</span><span>,</span> <span>delta_nabla_b</span><span>)]</span>
            <span>nabla_w</span> <span>=</span> <span>[</span><span>nw</span><span>+</span><span>dnw</span> <span>for</span> <span>nw</span><span>,</span> <span>dnw</span> <span>in</span> <span>zip</span><span>(</span><span>nabla_w</span><span>,</span> <span>delta_nabla_w</span><span>)]</span>
        <span>self</span><span>.</span><span>weights</span> <span>=</span> <span>[</span><span>w</span><span>-</span><span>(</span><span>eta</span><span>/</span><span>len</span><span>(</span><span>mini_batch</span><span>))</span><span>*</span><span>nw</span> 
                        <span>for</span> <span>w</span><span>,</span> <span>nw</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>weights</span><span>,</span> <span>nabla_w</span><span>)]</span>
        <span>self</span><span>.</span><span>biases</span> <span>=</span> <span>[</span><span>b</span><span>-</span><span>(</span><span>eta</span><span>/</span><span>len</span><span>(</span><span>mini_batch</span><span>))</span><span>*</span><span>nb</span> 
                       <span>for</span> <span>b</span><span>,</span> <span>nb</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>biases</span><span>,</span> <span>nabla_b</span><span>)]<br clear="none"/><br clear="none"/><br clear="none"/></span></pre><div><br clear="none"/></div><div><br clear="none"/></div><pre xml:space="preserve"><span><br clear="none"/></span></pre><pre xml:space="preserve"><span>class</span> <span>Network</span><span>(</span><span>object</span><span>):</span>
<span>...</span>
   <span>def</span> <span>backprop</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>y</span><span>):</span>
        <span>&quot;&quot;&quot;Return a tuple &quot;(nabla_b, nabla_w)&quot; representing the</span>
<span>        gradient for the cost function C_x.  &quot;nabla_b&quot; and</span>
<span>        &quot;nabla_w&quot; are layer-by-layer lists of numpy arrays, similar</span>
<span>        to &quot;self.biases&quot; and &quot;self.weights&quot;.&quot;&quot;&quot;</span>
        <span>nabla_b</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>shape</span><span>)</span> <span>for</span> <span>b</span> <span>in</span> <span>self</span><span>.</span><span>biases</span><span>]</span>
        <span>nabla_w</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>zeros</span><span>(</span><span>w</span><span>.</span><span>shape</span><span>)</span> <span>for</span> <span>w</span> <span>in</span> <span>self</span><span>.</span><span>weights</span><span>]</span>
        <span># feedforward</span>
        <span>activation</span> <span>=</span> <span>x</span>
        <span>activations</span> <span>=</span> <span>[</span><span>x</span><span>]</span> <span># list to store all the activations, layer by layer</span>
        <span>zs</span> <span>=</span> <span>[]</span> <span># list to store all the z vectors, layer by layer</span>
        <span>for</span> <span>b</span><span>,</span> <span>w</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>biases</span><span>,</span> <span>self</span><span>.</span><span>weights</span><span>):</span>
            <span>z</span> <span>=</span> <span>np</span><span>.</span><span>dot</span><span>(</span><span>w</span><span>,</span> <span>activation</span><span>)</span><span>+</span><span>b</span>
            <span>zs</span><span>.</span><span>append</span><span>(</span><span>z</span><span>)</span>
            <span>activation</span> <span>=</span> <span>sigmoid</span><span>(</span><span>z</span><span>)</span>
            <span>activations</span><span>.</span><span>append</span><span>(</span><span>activation</span><span>)</span>
        <span># backward pass</span>
        <span>delta</span> <span>=</span> <span>self</span><span>.</span><span>cost_derivative</span><span>(</span><span>activations</span><span>[</span><span>-</span><span>1</span><span>],</span> <span>y</span><span>)</span> <span>*</span> \
            <span>sigmoid_prime</span><span>(</span><span>zs</span><span>[</span><span>-</span><span>1</span><span>])</span>
        <span>nabla_b</span><span>[</span><span>-</span><span>1</span><span>]</span> <span>=</span> <span>delta</span>
        <span>nabla_w</span><span>[</span><span>-</span><span>1</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>dot</span><span>(</span><span>delta</span><span>,</span> <span>activations</span><span>[</span><span>-</span><span>2</span><span>]</span><span>.</span><span>transpose</span><span>())</span>
        <span># Note that the variable l in the loop below is used a little</span>
        <span># differently to the notation in Chapter 2 of the book.  Here,</span>
        <span># l = 1 means the last layer of neurons, l = 2 is the</span>
        <span># second-last layer, and so on.  It's a renumbering of the</span>
        <span># scheme in the book, used here to take advantage of the fact</span>
        <span># that Python can use negative indices in lists.</span>
        <span>for</span> <span>l</span> <span>in</span> <span>xrange</span><span>(</span><span>2</span><span>,</span> <span>self</span><span>.</span><span>num_layers</span><span>):</span>
            <span>z</span> <span>=</span> <span>zs</span><span>[</span><span>-</span><span>l</span><span>]</span>
            <span>sp</span> <span>=</span> <span>sigmoid_prime</span><span>(</span><span>z</span><span>)</span>
            <span>delta</span> <span>=</span> <span>np</span><span>.</span><span>dot</span><span>(</span><span>self</span><span>.</span><span>weights</span><span>[</span><span>-</span><span>l</span><span>+</span><span>1</span><span>]</span><span>.</span><span>transpose</span><span>(),</span> <span>delta</span><span>)</span> <span>*</span> <span>sp</span>
            <span>nabla_b</span><span>[</span><span>-</span><span>l</span><span>]</span> <span>=</span> <span>delta</span>
            <span>nabla_w</span><span>[</span><span>-</span><span>l</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>dot</span><span>(</span><span>delta</span><span>,</span> <span>activations</span><span>[</span><span>-</span><span>l</span><span>-</span><span>1</span><span>]</span><span>.</span><span>transpose</span><span>())</span>
        <span>return</span> <span>(</span><span>nabla_b</span><span>,</span> <span>nabla_w</span><span>)</span></pre><div>1. 输入 x:  设置输入层activation a</div><div>2. 正向更新: 对于l=1, 2, 3, .... L, 计算</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-26-30.png" type="image/png"/>     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-27-03.png" type="image/png"/></div><div>3. 计算出输出层error: </div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-28-10.png" type="image/png"/></div><div>4. 反向更新error (backpropagate error): </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-29-16.png" type="image/png"/></div><div>5. 输出: </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-30-16.png" type="image/png"/>       <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-30-39.png" type="image/png"/></div><pre xml:space="preserve">    <span>def</span> <span>cost_derivative</span><span>(</span><span>self</span><span>,</span> <span>output_activations</span><span>,</span> <span>y</span><span>):</span>
        <span>&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span>
<span>        \partial a for the output activations.&quot;&quot;&quot;</span>
        <span>return</span> <span>(</span><span>output_activations</span><span>-</span><span>y</span><span>)</span> 

<span>def</span> <span>sigmoid</span><span>(</span><span>z</span><span>):</span>
    <span>&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span>
    <span>return</span> <span>1.0</span><span>/</span><span>(</span><span>1.0</span><span>+</span><span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>z</span><span>))</span>

<span>def</span> <span>sigmoid_prime</span><span>(</span><span>z</span><span>):</span>
    <span>&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span>
    <span>return</span> <span>sigmoid</span><span>(</span><span>z</span><span>)</span><span>*</span><span>(</span><span>1</span><span>-</span><span>sigmoid</span><span>(</span><span>z</span><span>))</span></pre><pre xml:space="preserve"><span><br clear="none"/><br clear="none"/></span>为什么Backpropagation算法快?<br clear="none"/><br clear="none"/>假设为了求 ∂C/∂w 和 ∂C/∂b <br clear="none"/>1. 用微积分中的乘法法则,太复杂<br clear="none"/>2. 把Cost函数当做只是权重weight的函数, 于是定义:<br clear="none"/><br clear="none"/>     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 19-32-19.png" type="image/png"/> <br clear="none"/></pre><div>    一个很小的</div><div>         <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 19-33-17.png" type="image/png"/> </div><div>  单位向量: </div><div>                   <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 19-34-23.png" type="image/png"/></div><div> 看似可以, 但是具体计算时, 假设我们的神经网络中有1,000,000个权重,</div><div> 对于每一个权重weight, 我们都需要通过遍历一遍神经网络来计算  </div><div>     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-20 19-36-22.png" type="image/png"/><br clear="none"/></div><div><img src="https://www.evernote.com/shard/s27/res/96dc17cd-418d-41b8-8fb4-2e362e8ab655/screenshot-neuralnetworksanddeeplearning.com%202015-09-19%2018-00-28.png"></img></div><div> 对于1,000,000个权重, 我们需要遍历神经网络1,000,000次! 仅仅对于1个训练实  例x和y</div><div><br clear="none"/></div><div> Backpropagation算法的优势在于让我们在一正一反遍历一遍神经网络的时候, 就可以把所有的偏导数计算出来: ∂C/∂w (对于所有的w) </div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="521"/>

<div>
<span><div>总结四个方程:</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-18-40.png" type="image/png"/></div><div><br clear="none"/></div><div><div><strong>Backpropagation的四个关键公式:</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-02-09.png" type="image/png"/>定义error在l层,第j个神经元</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-05-49.png" type="image/png"/></div><div>坏蛋尝试破坏神经网络的计算, 增添了一个变化: </div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-07-34.png" type="image/png"/></div><div>本来输出 <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-08-52.png" type="image/png"/>, 结果输出 <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-09-34.png" type="image/png"/>, 最终造成Cost变化为: <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-10-36.png" type="image/png"/></div><div><br clear="none"/></div><div>假设现在这个坏蛋想帮我们, 通过找到一个<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-07-34 [1].png" type="image/png"/>来降低cost, 如果<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-14-41.png" type="image/png"/>太大, 坏蛋想帮我们降低cost, 通过找到合适的<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-07-34 [2].png" type="image/png"/>, 如果cost接近0, 无法改进太多, 接近最优, 所以</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-14-41 [1].png" type="image/png"/> 可以作为error的一个衡量,</div><div>于是,我们定义;</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-24-50.png" type="image/png"/></div><div><strong>一个对于error在输出层的方程:</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-30-50.png" type="image/png"/></div><div>等式右边第一项衡量Cost变化快对于第j个activation输出, 比如, 我们理想的情况是C不因为某一个特定的输出神经元而变化太大, 所以error就比较小. </div><div><br clear="none"/></div><div>等式右边第二部分是衡量activation方程变化对于</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-34-42.png" type="image/png"/></div><div>转化为矩阵的表达方式: </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-37-10.png" type="image/png"/></div><div>其中<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-37-39.png" type="image/png"/>是C根据输出层activation的变化率, 对于我们的二次Cost方程:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-39-22.png" type="image/png"/></div><div><br clear="none"/></div><div><strong>一个因下一层error变化引起的当前层error变化的方程:</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-42-53.png" type="image/png"/></div><div>下一层的error乘以权重矩阵的转置, 理解为error往回传递. 再求与</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-47-01.png" type="image/png"/>的Hadamard product, 算出对于l层的error</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>交替使用:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-30-50 [1].png" type="image/png"/></div><div>和<br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-42-53 [1].png" type="image/png"/></div><div>可以通过神经网络逐层往计算所有error.  BP1用来计算输出曾error, BP2用来计算前一层, BP1=&gt;BP2=&gt;BP1.....</div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>一个关于cost变化率根据偏向bias的方程:</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-56-15.png" type="image/png"/>根据BP1, BP2</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-57-34.png" type="image/png"/></div><div><br clear="none"/></div><div>一个关于cost比变化率根据权重weight的方程:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-59-46.png" type="image/png"/></div><div>这个告诉我们如何根据error和a求偏导数, error和a我们都可以求得</div><div>简化为:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-03-04.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-03-56.png" type="image/png"/></div><div>当a_in很小时,偏导数也很小,所以权重较慢学习, 结论就是从low activation算出的权重学习比较慢</div><div><br clear="none"/></div><div>根据</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 19-30-50 [2].png" type="image/png"/></div><div>和sigmoid函数的图像</div><div><br clear="none"/></div><div><img src="https://www.evernote.com/shard/s27/res/71e95f6b-f43a-4f8b-b89a-ed4fd955a9c9/sigGraph.png"></img></div><div>可以看出, 函数值接近1或者0的时候曲线都很平, 说明</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-15-21.png" type="image/png"/><br clear="none"/></div><div>接近0, 结论就是当输出层的权重学习较慢,如果输出层的activion很高或者很低时,对于bias偏好也一样</div><div><br clear="none"/></div><div>总结四个方程:</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-18-40 [1].png" type="image/png"/></div><div><br clear="none"/></div></div><div><br clear="none"/></div><div>Backpropatation 算法;</div><div><br clear="none"/></div><div>1. 输入 x:  设置输入层activation a</div><div>2. 正向更新: 对于l=1, 2, 3, .... L, 计算</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-26-30 [1].png" type="image/png"/>     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-27-03 [1].png" type="image/png"/></div><div>3. 计算出输出层error: </div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-28-10 [1].png" type="image/png"/></div><div>4. 反向更新error (backpropagate error): </div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-29-16 [1].png" type="image/png"/></div><div>5. 输出: </div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-30-16 [1].png" type="image/png"/>       <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 20-30-39 [1].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><div>Backpropagation算法</div><div>     5.1 通过迭代性的来处理训练集中的实例</div><div>     5.2 对比经过神经网络后输入层预测值(predicted value)与真实值(target value)之间</div><div>     5.3 反方向（从输出层=&gt;隐藏层=&gt;输入层）来以最小化误差(error)来更新每个连接的权重(weight)</div><div>     5.4 算法详细介绍</div><div>           输入：D：数据集，l 学习率(learning rate)， 一个多层前向神经网络</div><div>           输出：一个训练好的神经网络(a trained neural network)</div><div><br clear="none"/></div><div>          5.4.1 初始化权重(weights)和偏向(bias): 随机初始化在-1到1之间，或者-0.5到0.5之间，每个单元有          </div><div>                    一个偏向</div><div>          5.4.2 对于每一个训练实例X，执行以下步骤：</div><div>                    5.4.2.1： 由输入层向前传送</div><div>                                   <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [47].png" type="image/png" style="cursor: default;"/></div><div>                                   <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [48].png" type="image/png" style="cursor: default;"/></div><div>                    </div><div>                                   <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [49].png" type="image/png" style="cursor: default;"/></div><div><br clear="none"/></div><div>                                   <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [50].png" type="image/png" style="cursor: default;"/></div><div>                      5.4.2.2 根据误差(error)反向传送</div><div>                                   对于输出层：<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [51].png" type="image/png" style="cursor: default;"/></div><div>                                   对于隐藏层：</div><div>                                                       <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [52].png" type="image/png" style="cursor: default;"/></div><div>                                   权重更新：  <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [53].png" type="image/png" style="cursor: default;"/></div><div>                                   偏向更新    </div><div>                                                     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [54].png" type="image/png" style="cursor: default;"/></div><div>               5.4.3 终止条件</div><div>                         5.4.3.1 权重的更新低于某个阈值</div><div>                         5.4.3.2 预测的错误率低于某个阈值</div><div>                         5.4.3.3 达到预设一定的循环次数</div><div>                         </div><div>6. Backpropagation 算法举例</div><div>          </div><div>          <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [55].png" type="image/png" style="cursor: default;"/></div><div>                                   对于输出层：<img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [56].png" type="image/png" style="cursor: default;"/></div><div>                                   对于隐藏层：</div><div>                                                       <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [57].png" type="image/png" style="cursor: default;"/></div><div>                                   权重更新：  <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [58].png" type="image/png" style="cursor: default;"/></div><div>                                   偏向更新    </div><div>                                                     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [59].png" type="image/png" style="cursor: default;"/></div><div>          <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [60].png" type="image/png" style="cursor: default;"/></div></div></span>
</div>
<hr>
<a name="560"/>

<div>
<span><div><br clear="none"/></div><div><pre xml:space="preserve"><span>接着上节课:<br clear="none"/><br clear="none"/>平均灰度来衡量(Average Darkness)<br clear="none"/>SVM: 9,435 of 10,000<br clear="none"/><br clear="none"/><br clear="none"/>之前没有解释 backpropagation 算法<br clear="none"/><br clear="none"/>在1970年就被提出, 1986年的David Rumelhart, Geoffrey Hinton, and Ronald Williams提出的论文才得到重视, 可以解决神经网络中的学习<br clear="none"/><br clear="none"/>Backpropagation核心解决的问题: <span>∂</span><span>C</span><span><span><span>/</span></span></span><span>∂</span><span>w 和 <span>∂</span><span>C</span><span><span><span>/</span></span></span><span>∂b 的计算, 针对cost函数C<br clear="none"/></span></span></span><br clear="none"/><strong>标记</strong>:</pre></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 17-56-54.png" type="image/png"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-00-28.png" type="image/png"/> 对于a, b标记</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-04-19.png" type="image/png"/></div><div>两步: 1. wa+b, 2. sigmoid函数</div><div><br clear="none"/></div><div>对于每一层(l), 定义一个weight matrix (权重矩阵):                   </div><div>                     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-13-35.png" type="image/png"/></div><div>连接l层和l-1层, 矩阵中第j行,第k列的元素就是: </div><div>                      <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-19-42.png" type="image/png"/></div><div>对于每一层(l), 定义一个bias vector:</div><div>                       <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-23-15.png" type="image/png"/><span><span><span><br clear="none"/>表示当前层的每一个元素的bias是: <span><br clear="none"/>                        </span></span></span></span><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-24-44.png" type="image/png"/></div><div>同理, 对于a:   </div><div>                    <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-27-17.png" type="image/png"/>     <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-29-11.png" type="image/png"/></div><div>Vectorizing a function:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-33-05.png" type="image/png"/>     <br clear="none"/></div><div>例如: f(x) = x^2</div><div> <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-34-59.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-04-19 [1].png" type="image/png"/> =&gt; <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-36-23.png" type="image/png"/></div><div>用矩阵和向量表示简单很多, 对于每一层,只需要乘以权重的矩阵,加上偏向的向量</div><div><br clear="none"/></div><div>中间变量: <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-40-53.png" type="image/png"/>, <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-41-48.png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>关于Cost函数的两个假设:</strong></div><div><br clear="none"/></div><div>回顾Cost函数: </div><div>                           <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-43-28.png" type="image/png"/></div><div>L: 神经网络的层数</div><div>求和是对于所有单个的训练实例x加起来, 然后求平均Cost</div><div><br clear="none"/></div><div>假定1:  Cost函数可以写成如下:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-46-09.png" type="image/png"/>平均cost        <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-48-08.png" type="image/png"/> 对于单个x的Cost</div><div><br clear="none"/></div><div><br clear="none"/></div><div>因为对于backpropagation, ∂C/∂w 和 ∂C/∂b 的计算是通过单个实例x完成的</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-52-20.png" type="image/png"/></div><div>Cost可以被写成神经网络输出的一个函数</div><div><br clear="none"/></div><div>我们定义的这个二次cost方程满足这点: </div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-54-23.png" type="image/png"/></div><div><strong>The Hadamard product, s⊙t:</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-19 18-57-36.png" type="image/png"/>  元素分别相乘</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="793"/>

<div>
<span><div><strong>神经网络结构:</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [61].png" type="image/png"/></div><div><br clear="none"/></div><div>输入层                  隐藏层                输出层</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [62].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>两个隐藏层的神经网络</div><div>MultiLayer Perceptions (MLP): 实际是sigmoid neurons, 不是perceptrons</div><div><br clear="none"/></div><div> 假设识别一个手写图片:</div><div><br clear="none"/></div><div><img src="https://www.evernote.com/shard/s27/res/e3009f2b-229d-42d2-9c09-951e1b9fbcc6.png"></img></div><div><br clear="none"/></div><div>如果图片是64*64, 输入层总共有64*64 = 4096个神经元</div><div>如果图片是28*28, 输入层总共有28*28 = 784个神经元</div><div><br clear="none"/></div><div>如果输出层只有一个神经元, &gt;0.5说明是9, &lt;0.5说明不是9</div><div><br clear="none"/></div><div>FeedForward Network: 神经网络中没有循环, 信息单项向前传递</div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>用神经网络识别手写数字:</strong></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [63].png" type="image/png" width="300px"/></div><div><br clear="none"/></div><div>分层:  segmentation分层 得到:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [64].png" type="image/png" width="440px"/></div><div><br clear="none"/></div><div>对于第一个数字:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [65].png" type="image/png" width="64px"/></div><div><br clear="none"/></div><div>用以下神经网络识别:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [66].png" type="image/png"/></div><div>输入层: 28x28 = 784个神经元</div><div>每个神经元代表一个像素的值:0.0全白,1.0全黑</div><div><br clear="none"/></div><div>一个隐藏层: n个神经元, 例子中 n=15</div><div><br clear="none"/></div><div>输出层: 10个神经元,分别代表手写数字识别可能的0~9十个数字,</div><div>例如: 第一个神经元(代表0)的输出值=1, 其他的&lt;1, 数字被识别为0</div><div><br clear="none"/></div><div>隐藏层学习到不同的部分:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [67].png" type="image/png" width="130px"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [68].png" type="image/png" width="424px"/></div><div><br clear="none"/></div><div>决定:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [69].png" type="image/png" width="130px"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>还可能有很多其他方式做出最终分类决定,所以使用10个神经元在输入层</div><div><br clear="none"/></div><div><br clear="none"/></div><div>梯度下降(gradient descent):</div><div><br clear="none"/></div><div>MNIST dataset: <a href="http://yann.lecun.com/exdb/mnist/" shape="rect" target="_blank">http://yann.lecun.com/exdb/mnist/</a></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [70].png" type="image/png" width="420px"/></div><div><br clear="none"/></div><div>得名: Modified National Institute of Standards and Technology </div><div><br clear="none"/></div><div>训练数据集: 60,000 张图片 =&gt; 用来训练</div><div>测试数据集: 10, 000 张图片 =&gt; 用来测试准确率</div><div><br clear="none"/></div><div>扫描从250个员工的手写字体而来</div><div><br clear="none"/></div><div>x: 训练输入, 28*28 = 784d向量, 每个值代表一个灰度图片中的一个像素值</div><div>y=y(x): 10d向量</div><div><br clear="none"/></div><div>如果输入的某个图片是数字6</div><div>理想输出: <span>y</span><span>(</span><span>x</span><span>)</span><span>=</span><span>(</span><span>0</span><span>,</span><span>0</span><span>,</span><span>0</span><span>,</span><span>0</span><span>,</span><span>0</span><span>,</span><span>0</span><span>,</span><span>1</span><span>,</span><span>0</span><span>,</span><span>0</span><span>,</span><span>0</span><span><span><span><span>)</span></span><span><span>T</span></span></span></span></div><div><span><br clear="none"/></span></div><div>Cost function (loss function, objective function): 目标函数</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-13 20-57-03.png" type="image/png"/></div><div>C: cost</div><div>w: weight 权重</div><div>b: bias 偏向</div><div>n: 训练数据集实例个数</div><div>x: 输入值</div><div>a: 输出值 (当x是输入时)</div><div>||v||:  向量的length function</div><div><br clear="none"/></div><div><br clear="none"/></div><div>C(w,b) 越小越好，输出的预测值和真实值差别越小</div><div><br clear="none"/></div><div>目标： 最小化C(w,b)</div><div><br clear="none"/></div><div><br clear="none"/></div><div>最小化问题可以用梯度下降解决（gradient descent)</div><div><br clear="none"/></div><div>C(v) v有两个变量v1, v2</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [71].png" type="image/png" width="542px"/></div><div><br clear="none"/></div><div>通常可以用微积分解决，如果v包含的变量过多，无法用微积分解决.</div><div><br clear="none"/></div><div>假设一个小球在曲面上的某一点，滚动到最低点</div><div><br clear="none"/></div><div><div>一个变量的情况:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [2].jpg" type="image/jpeg" alt="Image result for gradient descent"/></div><div>可能会陷入局部最优</div><div>前提是目标函数要是凸函数convex</div><div>learning rate自动会减小</div><div><br clear="none"/></div><div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [72].png" type="image/png" height="319" width="613"/><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-15 01-56-06 [1].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></div><div><br clear="none"/></div></div></span>
</div>
<hr>
<a name="831"/>

<div>
<span><div><br clear="none"/><br clear="none"/></div><div><pre xml:space="preserve"><span>def</span> <span>SGD</span><span>(</span><span>self</span><span>,</span> <span>training_data</span><span>,</span> <span>epochs</span><span>,</span> <span>mini_batch_size</span><span>,</span> <span>eta</span><span>,</span>
            <span>test_data</span><span>=</span><span>None</span><span>):</span>
        <span>&quot;&quot;&quot;Train the neural network using mini-batch stochastic</span>
<span>        gradient descent.  The &quot;training_data&quot; is a list of tuples</span>
<span>        &quot;(x, y)&quot; representing the training inputs and the desired</span>
<span>        outputs.  The other non-optional parameters are</span>
<span>        self-explanatory.  If &quot;test_data&quot; is provided then the</span>
<span>        network will be evaluated against the test data after each</span>
<span>        epoch, and partial progress printed out.  This is useful for</span>
<span>        tracking progress, but slows things down substantially.&quot;&quot;&quot;</span>
        <span>if</span> <span>test_data</span><span>:</span> <span>n_test</span> <span>=</span> <span>len</span><span>(</span><span>test_data</span><span>)</span>
        <span>n</span> <span>=</span> <span>len</span><span>(</span><span>training_data</span><span>)</span>
        <span>for</span> <span>j</span> <span>in</span> <span>xrange</span><span>(</span><span>epochs</span><span>):</span>
            <span>random</span><span>.</span><span>shuffle</span><span>(</span><span>training_data</span><span>)</span>
            <span>mini_batches</span> <span>=</span> <span>[</span>
                <span>training_data</span><span>[</span><span>k</span><span>:</span><span>k</span><span>+</span><span>mini_batch_size</span><span>]</span>
                <span>for</span> <span>k</span> <span>in</span> <span>xrange</span><span>(</span><span>0</span><span>,</span> <span>n</span><span>,</span> <span>mini_batch_size</span><span>)]</span>
            <span>for</span> <span>mini_batch</span> <span>in</span> <span>mini_batches</span><span>:</span>
                <span>self</span><span>.</span><span>update_mini_batch</span><span>(</span><span>mini_batch</span><span>,</span> <span>eta</span><span>)</span>
            <span>if</span> <span>test_data</span><span>:</span>
                <span>print</span> <span>&quot;Epoch {0}: {1} / {2}&quot;</span><span>.</span><span>format</span><span>(</span>
                    <span>j</span><span>,</span> <span>self</span><span>.</span><span>evaluate</span><span>(</span><span>test_data</span><span>),</span> <span>n_test</span><span>)</span>
            <span>else</span><span>:</span>
                <span>print</span> <span>&quot;Epoch {0} complete&quot;</span><span>.</span><span>format</span><span>(</span><span>j</span><span>)<br clear="none"/><br clear="none"/><img src="https://www.evernote.com/shard/s27/res/a5f9ae9a-86a9-4ed0-a406-aa3dcc63a26f/screenshot-neuralnetworksanddeeplearning.com%202015-09-15%2001-56-06.png"></img><br clear="none"/></span><br clear="none"/></pre></div><div><br clear="none"/></div><div><pre xml:space="preserve">   <span>def</span> <span>update_mini_batch</span><span>(</span><span>self</span><span>,</span> <span>mini_batch</span><span>,</span> <span>eta</span><span>):</span>
        <span>&quot;&quot;&quot;Update the network's weights and biases by applying</span>
<span>        gradient descent using backpropagation to a single mini batch.</span>
<span>        The &quot;mini_batch&quot; is a list of tuples &quot;(x, y)&quot;, and &quot;eta&quot;</span>
<span>        is the learning rate.&quot;&quot;&quot;</span>
        <span>nabla_b</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>shape</span><span>)</span> <span>for</span> <span>b</span> <span>in</span> <span>self</span><span>.</span><span>biases</span><span>]</span>
        <span>nabla_w</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>zeros</span><span>(</span><span>w</span><span>.</span><span>shape</span><span>)</span> <span>for</span> <span>w</span> <span>in</span> <span>self</span><span>.</span><span>weights</span><span>]</span>
        <span>for</span> <span>x</span><span>,</span> <span>y</span> <span>in</span> <span>mini_batch</span><span>:</span>
            <span>delta_nabla_b</span><span>,</span> <span>delta_nabla_w</span> <span>=</span> <span>self</span><span>.</span><span>backprop</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
            <span>nabla_b</span> <span>=</span> <span>[</span><span>nb</span><span>+</span><span>dnb</span> <span>for</span> <span>nb</span><span>,</span> <span>dnb</span> <span>in</span> <span>zip</span><span>(</span><span>nabla_b</span><span>,</span> <span>delta_nabla_b</span><span>)]</span>
            <span>nabla_w</span> <span>=</span> <span>[</span><span>nw</span><span>+</span><span>dnw</span> <span>for</span> <span>nw</span><span>,</span> <span>dnw</span> <span>in</span> <span>zip</span><span>(</span><span>nabla_w</span><span>,</span> <span>delta_nabla_w</span><span>)]</span>
        <span>self</span><span>.</span><span>weights</span> <span>=</span> <span>[</span><span>w</span><span>-</span><span>(</span><span>eta</span><span>/</span><span>len</span><span>(</span><span>mini_batch</span><span>))</span><span>*</span><span>nw</span> 
                        <span>for</span> <span>w</span><span>,</span> <span>nw</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>weights</span><span>,</span> <span>nabla_w</span><span>)]</span>
        <span>self</span><span>.</span><span>biases</span> <span>=</span> <span>[</span><span>b</span><span>-</span><span>(</span><span>eta</span><span>/</span><span>len</span><span>(</span><span>mini_batch</span><span>))</span><span>*</span><span>nb</span> 
                       <span>for</span> <span>b</span><span>,</span> <span>nb</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>biases</span><span>,</span> <span>nabla_b</span><span>)]<br clear="none"/></span><br clear="none"/></pre></div><div><img src="https://www.evernote.com/shard/s27/res/a5f9ae9a-86a9-4ed0-a406-aa3dcc63a26f/screenshot-neuralnetworksanddeeplearning.com%202015-09-15%2001-56-06.png"></img> <img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-16 14-49-00.png" type="image/png"/></div><div><br clear="none"/></div><div>下载代码:</div><div><a href="https://github.com/mnielsen/neural-networks-and-deep-learning" shape="rect" target="_blank">https://github.com/mnielsen/neural-networks-and-deep-learning</a></div><div><br clear="none"/></div><div>两种方式:</div><div>命令行</div><div>Eclipse</div><div><br clear="none"/></div><div><br clear="none"/></div><div>介绍mnist_loader</div><div><br clear="none"/></div><div><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>import</span> <span>mnist_loader</span>
<span>&gt;&gt;&gt;</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>,</span> <span>test_data</span> <span>=</span> \
<span>...</span> <span>mnist_loader</span><span>.</span><span>load_data_wrapper</span><span>()<br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/><br clear="none"/></span></pre><pre xml:space="preserve"><span><br clear="none"/></span><br clear="none"/></pre></div></span>
</div>
<hr>
<a name="833"/>

<div>
<span><div>目标函数:</div><div><br clear="none"/></div><div><img src="https://www.evernote.com/shard/s27/res/667296c9-cf9f-48b6-a2b6-b888afdf6555.png"></img></div><div><br clear="none"/></div><div>变化量:</div><div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-14 00-07-44.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-14 00-09-31.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-14 00-10-57.png" type="image/png"/></div><div>gradient vector</div><div><br clear="none"/></div><div>以上三个公式推出：</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-14 00-13-37.png" type="image/png"/></div><div>设定:</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-15 01-48-17.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-15 01-49-50.png" type="image/png"/>    &lt;= 0</div><div><br clear="none"/></div><div>所以C不断减小</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-15 01-51-27.png" type="image/png"/></div></div><div><br clear="none"/></div><div>回顾目标函数:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-16 14-12-10.png" type="image/png"/><br clear="none"/></div><div>是平均的cost</div><div><br clear="none"/></div><div>权重和偏向更新方程:</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-16 14-54-01.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-16 14-54-33.png" type="image/png"/></div><div><span><br clear="none"/></span></div><div>对于每个训练实例x, 都要j计算梯度向量gradient vector:  <span>∇</span><span>C</span></div><div><span>如果训练数据集过大, 会花费很长时间,学习过程太慢</span></div><div><br clear="none"/></div><div><span>所以, 一个变种称为:</span></div><div><span><br clear="none"/></span></div><div><strong><span>随机梯度下降算法 (stochastic gradient descent):</span></strong></div><div><br clear="none"/></div><div>基本思想: 从所有训练实例中取一个小的采样(sample): X1,X2,…,Xm   (mini-batch)</div><div>来估计<span> </span><span>∇</span><span>C, 大大提高学习速度</span></div><div><span><br clear="none"/></span></div><div><span>举例: 选举调查</span></div><div><span><br clear="none"/></span></div><div><span>如果样本够大, </span></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-16 14-42-58.png" type="image/png"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-16 14-43-26.png" type="image/png"/></div><div>代入更新方程:</div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/screenshot-neuralnetworksanddeeplearning.com 2015-09-16 14-49-00 [1].png" type="image/png"/></div><div>然后,重新选择一个mini-batch用来训练,直到用完所有的训练实例,一轮epoch完成</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="847"/>

<div>
<span><div><strong>实现一个手写数字识别的算法</strong></div><div><strong> </strong></div><div>MNIST数据集:</div><div>训练(train) : 50,000</div><div>验证(validation): 10,000 </div><div>测试(test): 10,000</div><div><img src="https://www.evernote.com/shard/s27/res/95996359-bba8-42fd-98e5-9d6d199cb254.png"></img></div><div><pre xml:space="preserve">class Network(object):

    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) 
                        for x, y in zip(sizes[:-1], sizes[1:])]<br clear="none"/><br clear="none"/>sizes: 每层神经元的个数, 例如: 第一层2个神经元,第二层3个神经元: </pre><pre xml:space="preserve">net = Network([2, 3, 1])<br clear="none"/></pre></div><div><br clear="none"/>np.random.rand(y, 1): 随机从正态分布(均值0, 方差1)中生成</div><div><br clear="none"/></div><div><span>net.weights[1] 存储连接第二层和第三层的权重 (Python索引从0开始数)</span></div><div><br clear="none"/></div><div><span><span><span>a</span><span>′</span></span><span>=</span><span>σ</span><span>(</span><span>w</span><span>a</span><span>+</span><span>b</span><span>)</span></span></div><div><br clear="none"/></div><div><pre xml:space="preserve">    <span>def</span> <span>feedforward</span><span>(</span><span>self</span><span>,</span> <span>a</span><span>):</span>
        <span>&quot;&quot;&quot;Return the output of the network if &quot;a&quot; is input.&quot;&quot;&quot;</span>
        <span>for</span> <span>b</span><span>,</span> <span>w</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>biases</span><span>,</span> <span>self</span><span>.</span><span>weights</span><span>):</span>
            <span>a</span> <span>=</span> <span>sigmoid</span><span>(</span><span>np</span><span>.</span><span>dot</span><span>(</span><span>w</span><span>,</span> <span>a</span><span>)</span><span>+</span><span>b</span><span>)</span>
        <span>return</span> <span>a</span></pre></div><div><br clear="none"/></div><div><br clear="none"/></div><div><pre xml:space="preserve"><span><br clear="none"/></span><br clear="none"/></pre><pre xml:space="preserve"><br clear="none"/></pre></div></span>
</div>
<hr>
<a name="848"/>

<div>
<span><pre xml:space="preserve">总体复习network代码<br clear="none"/><br clear="none"/>演示:<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve">&gt;&gt;&gt; import network
&gt;&gt;&gt; net = network.Network([784, 30, 10])<br clear="none"/></pre><pre xml:space="preserve">&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data)</pre><pre xml:space="preserve"><br clear="none"/><br clear="none"/></pre><pre xml:space="preserve">&gt;&gt;&gt; net = network.Network([784, 100, 10])
&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data)<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>100</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>0.001</span><span>,</span> <span>test_data</span><span>=</span><span>test_data</span><span>)<br clear="none"/><br clear="none"/></span></pre><pre xml:space="preserve"><span>&gt;&gt;&gt;</span> <span>net</span> <span>=</span> <span>network</span><span>.</span><span>Network</span><span>([</span><span>784</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>])</span>
<span>&gt;&gt;&gt;</span> <span>net</span><span>.</span><span>SGD</span><span>(</span><span>training_data</span><span>,</span> <span>30</span><span>,</span> <span>10</span><span>,</span> <span>100.0</span><span>,</span> <span>test_data</span><span>=</span><span>test_data</span><span>)<br clear="none"/><br clear="none"/>平均灰度来衡量(Average Darkness)<br clear="none"/><br clear="none"/>SVM: <span>9,435 of 10,000</span><br clear="none"/><br clear="none"/>每年的记录: http://yann.lecun.com/exdb/mnist/<br clear="none"/><br clear="none"/><br clear="none"/></span></pre><pre xml:space="preserve"><br clear="none"/></pre></span>
</div>
<hr>
<a name="872"/>

<div>
<span><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>人脑识别图像:</strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [3].jpg" type="image/jpeg" width="600"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [73].png" type="image/png" width="160px"/><br clear="none"/></div><div><br clear="none"/></div><div>人脑: 大脑视觉皮层V1, 包含140,000,000神经元; 数百亿个连接, 还有V2,V3,V4,V5</div><div>         人脑进化几千年</div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [4].jpg" type="image/jpeg" width="600"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>计算机识别图像:</strong></div><div><strong><br clear="none"/></strong></div><div><strong><br clear="none"/></strong></div><div><strong><br clear="none"/></strong></div><div><strong><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [74].png" type="image/png" height="396" width="763"/></strong></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [75].png" type="image/png" width="440px"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [76].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><strong>神经元:</strong></div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [77].png" type="image/png"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Neural networks and deep learning.png" type="image/png"/></div><div><br clear="none"/></div><div>今天去不去踢球?</div><div><br clear="none"/></div><div>天气冷吗?</div><div>跟我常配合的好友去吗?</div><div>是不是晚饭前?</div><div><br clear="none"/></div><div>w1 = 2, w2 = 6, w3 = 3, threshold = 5</div><div><br clear="none"/></div><div><br clear="none"/></div><div>实际模型更加复杂:</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [78].png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/>￼<br clear="none"/></div><div>w,x 是向量, b=-threshold</div><div>b:多容易输出1,或者大脑多容易触发这个神经元</div><div><br clear="none"/></div><div>相当于与非门 (NAND gate):</div><div><br clear="none"/></div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [79].png" type="image/png"/> </div><div><span>输入00, (</span><span>−</span><span>2</span><span>)</span><span>∗</span><span>0</span><span>+</span><span>(</span><span>−</span><span>2</span><span>)</span><span>∗</span><span>0</span><span>+</span><span>3</span><span>=</span><span>3,    输出1</span></div><div><span>输入11, <span>(</span><span>−</span><span>2</span><span>)</span><span>∗</span><span>1</span><span>+</span><span>(</span><span>−</span><span>2</span><span>)</span><span>∗</span><span>1</span><span>+</span><span>3</span><span>=</span><span>−</span><span>1, 输出0</span></span></div><div><span><span><br clear="none"/></span></span></div><div>与非门可以模拟任何方程!</div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><span><span><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [80].png" type="image/png"/></span></span></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div>Sigmoid 神经元:</div><div><br clear="none"/></div><div><br clear="none"/></div><div><span><span><br clear="none"/></span></span></div><div><span><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [81].png" type="image/png"/></span></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><span>8-&gt;9</span></div><div><br clear="none"/></div><div>为了模拟更细微的变化,输入和输出数值从0和1,到0,1之间的任何数</div><div><br clear="none"/></div><div><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/sigmoidF.png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/sigmoidF2.png" type="image/png"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/sigGraph.png" type="image/png"/></div><div><br clear="none"/><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/sigGraph2.png" type="image/png"/></div></span>
</div>
<hr>
<a name="889"/>

<div>
<span><div>1. 常用软件包：</div><div><br clear="none"/></div><div>Theano</div><div>Pylearn2</div><div>scikit-neuralnetwork</div><div><br clear="none"/></div><div>Caffe</div><div><br clear="none"/></div><div>Deeplearning4j</div><div><br clear="none"/></div><div>Torch<br clear="none"/></div><div><br clear="none"/></div><div><a href="http://deeplearning.net/software_links/" shape="rect" target="_blank">http://deeplearning.net/software_links/</a></div><div><br clear="none"/></div><div>2. 环境配置</div><div><br clear="none"/></div><div>Linux: Ubuntu</div><div><br clear="none"/></div><div>Eclipse</div><div><br clear="none"/></div><div>PyDev</div><div><br clear="none"/></div><div>Python</div><div><br clear="none"/></div><div>CUDA</div><div><br clear="none"/></div><div>GPU: <a href="https://developer.nvidia.com/cuda-gpus" shape="rect" target="_blank">https://developer.nvidia.com/cuda-gpus</a></div><div><br clear="none"/></div><div>3. 神经网络算法 (neural networks)</div><div><br clear="none"/></div><div><a href="http://www.maiziedu.com/course/python/373-3811/" shape="rect" target="_blank">http://www.maiziedu.com/course/python/373-3811/</a></div><div>12, 14, 15</div><div><br clear="none"/></div><div>4. scikit-learn </div><div><br clear="none"/></div><div><a href="http://scikit-learn.org/stable/" shape="rect" target="_blank">http://scikit-learn.org/stable/</a></div><div><br clear="none"/></div><div>5. scikit-neuralnetwork</div><div><br clear="none"/></div><div><a href="https://github.com/aigamedev/scikit-neuralnetwork" shape="rect" target="_blank">https://github.com/aigamedev/scikit-neuralnetwork</a></div><div><br clear="none"/></div><div>5.1 安装dependencies:</div><div><br clear="none"/></div><div><pre xml:space="preserve">&gt; pip install numpy scipy theano
&gt; pip install -e git+https://github.com/lisa-lab/pylearn2.git#egg=Package<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve">&gt; git clone https://github.com/aigamedev/scikit-neuralnetwork.git
&gt; cd scikit-neuralnetwork; python setup.py develop<br clear="none"/><br clear="none"/><br clear="none"/></pre></div><div>5.2 安装scikit-neuralnetwork</div><div><br clear="none"/></div><div><pre xml:space="preserve">&gt; pip install scikit-neuralnetwork
</pre><div><br clear="none"/></div><div>5.3 测试：</div><div><br clear="none"/></div><div><pre xml:space="preserve">&gt; pip install nose
&gt; nosetests -v sknn.tests<br clear="none"/><br clear="none"/>如果出现报错： ImportError: No module named dnn<br clear="none"/><br clear="none"/>请确保更新theano: </pre></div></div><div>sudo pip install theano --upgrade</div><div><br clear="none"/></div><div>5.4 MNIST数据集：</div><div><a href="http://yann.lecun.com/exdb/mnist/" shape="rect">http://yann.lecun.com/exdb/mnist/</a></div><div><br clear="none"/></div><div>5.6 Demo</div><div><br clear="none"/></div><div><pre>&gt; python examples/bench_mnist.py (sknn|lasagne)
</pre><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="890"/>

<div>
<span><div><br clear="none"/></div><div>OS: Linux: Ubuntu 14.04</div><div><br clear="none"/></div><div>安装：</div><div><br clear="none"/></div><div>1. Pip (Python 2.7.9或以上自带pip）： sudo apt-get install pyton-pip</div><div><br clear="none"/></div><div>2. 尝试安装scikit-neuralnetwork: 需要 numpy scipy theano</div><div><br clear="none"/></div><div>sudo pip install scikit-neuralnetwork</div><div>错误：SystemError: cannot compile &quot;python.h&quot;. Perhaps you need to install python-dev|python-devel<br clear="none"/></div><div><br clear="none"/></div><div>解决方法：</div><div>sudo apt-get update</div><div>sudo apt-get upgrade gcc<br clear="none"/></div><div>sudo apt-get install python2.7-dev <br clear="none"/></div><div><br clear="none"/></div><div>3. 安装numpy, scipy, theano：</div><div>pip install numpy scipy theano</div><div>sudo pip install numpy scipy theano (1:03:32)<br clear="none"/></div><div><br clear="none"/>错误：numpy.distutils.system_info.NotFoundError: no lapack/blas resources found</div><div>解决方法：sudo apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran</div><div><div><br clear="none"/></div></div><div>sudo pip install scipy</div><div>sudo pip install Theano</div><div><br clear="none"/></div><div>4. 安装Pylearn2</div><div><br clear="none"/></div><div>sudo pip install -e git+https://github.com/lisa-lab/pylearn2.git#egg=Package</div><div>需要git</div><div>sudo apt-get install git</div><div><br clear="none"/></div><div>5. 安装scikit-neuralnetwork</div><div><br clear="none"/></div><div><pre xml:space="preserve">&gt; git clone https://github.com/aigamedev/scikit-neuralnetwork.git
&gt; cd scikit-neuralnetwork; python setup.py develop<br clear="none"/><br clear="none"/>6. 测试<br clear="none"/><br clear="none"/></pre><pre xml:space="preserve">&gt; sudo pip install nose
&gt; nosetests -v sknn.tests<br clear="none"/><br clear="none"/>需要安装matplotlib: sudo pip install matplotlib<br clear="none"/><br clear="none"/>错误：The following package could not be built: freetype, png<br clear="none"/>解决方法：sudo apt-get install libpng-dev<br clear="none"/>         sudo apt-get install libjpeg8-dev<br clear="none"/>         sudo apt-get install libfreetype6-dev<br clear="none"/><br clear="none"/>7. 视觉化显示：<strong>﻿</strong></pre><pre xml:space="preserve">&gt; python examples/plot_mlp.py --params activation<br clear="none"/><br clear="none"/>8. 在MNIST上测试</pre><pre xml:space="preserve">&gt; python examples/bench_mnist.py (sknn|lasagne)</pre><pre xml:space="preserve"><br clear="none"/><br clear="none"/> </pre><pre xml:space="preserve"><br clear="none"/><br clear="none"/></pre></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div><div><br clear="none"/></div></span>
</div>
<hr>
<a name="944"/>

<div>
<span style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;"><div style="font-size: 16px;"><font face="Times New Roman">1. 什么是深度学习？</font></div><div style="font-size: 16px;"><font face="Times New Roman">          深度学习是基于机器学习延伸出来的一个新的领域，由以人大脑结构为启发的神经网络算法为起源加之模型结构深度的增加发展，并伴随大数据和计算能力的提高而产生的一系列新的算法。</font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman">2. 深度学习什么时间段发展起来的？</font></div><div style="font-size: 16px;"><font face="Times New Roman">          其概念由著名科学家Geoffrey Hinton等人在2006年和2007年在《Sciences》等上发表的文章被提出和兴起。</font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman">3. 深度学习能用来干什么？为什么近年来引起如此广泛的关注？</font></div><div style="font-size: 16px;"><font face="Times New Roman">          深度学习，作为机器学习中延伸出来的一个领域，被应用在图像处理与计算机视觉，自然语言处理以及语音识别等领域。自2006年至今，学术界和工业界合作在深度学习方面的研究与应用在以上领域取得了突破性的进展。以ImageNet为数据库的经典图像中的物体识别竞赛为例，击败了所有传统算法，取得了前所未有的精确度。</font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman">4. 深度学习目前有哪些代表性的学术机构和公司走在前沿？人才需要如何？</font></div><div style="font-size: 16px;"><font face="Times New Roman">           学校以多伦多大学，纽约大学，斯坦福大学为代表，工业界以Google, Facebook, 和百度为代表走在深度学习研究与应用的前沿。Google挖走了Hinton，Facebook挖走了LeCun，百度硅谷的实验室挖走了Andrew Ng，Google今年4月份以超过5亿美金收购了专门研究深度学习的初创公司DeepMind, 深度学习方因技术的发展与人才的稀有造成的人才抢夺战达到了前所未有激烈的程度。诸多的大大小小(如阿里巴巴，雅虎）等公司也都在跟进，开始涉足深度学习领域，深度学习人才需求量会持续快速增长。</font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman">5. 深度学习如今和未来将对我们生活造成怎样的影响？</font></div><div style="font-size: 16px;"><font face="Times New Roman">           目前我们使用的Android手机中google的语音识别，百度识图，google的图片搜索，都已经使用到了深度学习技术。Facebook在去年名为DeepFace的项目中对人脸识别的准备率第一次接近人类肉眼（97.25% vs 97.5%)。大数据时代，结合深度学习的发展在未来对我们生活的影响无法估量。保守而言，很多目前人类从事的活动都将因为深度学习和相关技术的发展被机器取代，如自动汽车驾驶，无人飞机，以及更加职能的机器人等。深度学习的发展让我们第一次看到并接近人工智能的终极目标。</font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman">6. 深度学习范畴</font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [82].png" type="image/png" style="cursor: default;cursor: default;"/></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman">7. 深度学习基本模型</font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [83].png" type="image/png" style="cursor: default;cursor: default;"/></div><div style="font-size: 16px;"><br/></div><div style="font-size: 16px;">8. 深度学习与传统机器学习</div><div style="font-size: 16px;"><br/></div><div style="font-size: 16px;"><img src="Introduction to Deep Learning Advance Algorithm & Applications_files/Image [84].png" type="image/png" style="cursor: default;cursor: default;"/></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman"><br/></font></div><div style="font-size: 16px;"><font face="Times New Roman">Reference:</font></div><div style="font-size: 16px;"><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" style="color: rgb(102, 102, 102); font-weight: bold; text-decoration: none; font-family: tahoma, arial, sans-serif; font-size: 12.8000001907349px; text-align: left; background-color: rgb(255, 255, 255);">Deep Learning</a><span style="font-family: tahoma, arial, sans-serif; font-size: 12.8000001907349px; text-align: left; background-color: rgb(255, 255, 255);">, Yoshua Bengio, Ian Goodfellow, Aaron Courville, MIT Press</span></div></span>
</div>
<hr>
<a name="2614"/>

<div>
<span><ol><li><div><a href="#301" style="color:#69aa35">8.2 Restricted Boltzmann Machine 下</a></div></li><li><div><a href="#306" style="color:#69aa35">8.1 Restricted Boltzmann Machine</a></div></li><li><div><a href="#313" style="color:#69aa35">8.3 Deep Brief Network &amp; Autoencoder</a></div></li><li><div><a href="#315" style="color:#69aa35">7.3 Convolution Neural Network实现下</a></div></li><li><div><a href="#316" style="color:#69aa35">7.2 Convolution Neural Network 实现</a></div></li><li><div><a href="#322" style="color:#69aa35">7.1 Convolution Neural Network 算法</a></div></li><li><div><a href="#336" style="color:#69aa35">6.2 用ReL解决vanishing gradient问题</a></div></li><li><div><a href="#344" style="color:#69aa35">6.1 深度神经网络中的训练难点</a></div></li><li><div><a href="#391" style="color:#69aa35">5.7 神经网络参数(hyper-parameters)选择</a></div></li><li><div><a href="#398" style="color:#69aa35">5.6 实现提高版本的手写数字识别算法</a></div></li><li><div><a href="#404" style="color:#69aa35">5.5 正态分布, 初始化w,b</a></div></li><li><div><a href="#410" style="color:#69aa35">5.4 Regularization: L1, Dropout</a></div></li><li><div><a href="#432" style="color:#69aa35">5.3 Regularization: L2</a></div></li><li><div><a href="#450" style="color:#69aa35">5.2: Softmax和Overfitting</a></div></li><li><div><a href="#475" style="color:#69aa35">5.1 Cross-Entropy Cost</a></div></li><li><div><a href="#500" style="color:#69aa35">4.3 Backpropagation算法实现</a></div></li><li><div><a href="#521" style="color:#69aa35">4.2 Backpropagation算法下</a></div></li><li><div><a href="#560" style="color:#69aa35">4.1 Backpropagation算法上</a></div></li><li><div><a href="#793" style="color:#69aa35">3.2 神经网络基本结构, 梯度下降算法</a></div></li><li><div><a href="#831" style="color:#69aa35">3.5 梯度下降算法实现下</a></div></li><li><div><a href="#833" style="color:#69aa35">3.3 梯度下降算法的变种</a></div></li><li><div><a href="#847" style="color:#69aa35">3.4 梯度下降算法实现上</a></div></li><li><div><a href="#848" style="color:#69aa35">3.6 用神经网络识别手写数字演示</a></div></li><li><div><a href="#872" style="color:#69aa35">3.1 手写数字识别: MNIST, Perceptron</a></div></li><li><div><a href="#889" style="color:#69aa35">2.1 常用软件包和环境配置 (Software Packages &amp; Environment Configuration)</a></div></li><li><div><a href="#890" style="color:#69aa35">2.2 环境配置分部详解: pip, numpy, scikit-nerualnetwork</a></div></li><li><div><a href="#944" style="color:#69aa35">1.1 基本概念 (basic concepts)</a></div></li></ol></span>
</div></body></html> 